{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Introduction to Data Warehousing\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The concept of Data Warehousing originated at IBM in the 80's. The goal of the initial research was to provide a framework to transfer data from operational systems to business intelligence departments, avoiding the cost and technical challenges of high redundancy.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Analysts cannot work directly on business databases\n",
    "\n",
    "Business databases must stay clean at all cost: allowing Data Analysis or Data Scientist to access it introduces a breach \n",
    "\n",
    "Moreover, most of the times, unstructured data (ie, not stored in any kind of databases) is required to do performant analysis. \n",
    "\n",
    "A Warehousing solution allows the company to aggregate and store its data needed for analysis, without altering the databases used for operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Warehouse VS Data Lake\n",
    "\n",
    "You often hear both when discussing Big Data, however they are very different.\n",
    "\n",
    "Data Lakes are a big pool of raw data, with no defined purposes: we store this unstructured data in prevision of future usage.\n",
    "\n",
    "Data Warehouse holds **processed** and **structured** data, ready to be used for advanced analytics. \n",
    "\n",
    "Most of the time, data that ends up in the Warehouse was previously stored in the Lake. \n",
    "\n",
    "- Step 1: Data is collected and stored in its raw form in a Data Lake\n",
    "- Step 2: Data is extracted from the Lake, cleaned and processed\n",
    "- Step 3: Data is loaded in the warehouse, ready to be queried.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Warehouse VS traditional databases\n",
    "\n",
    "Roughly, a Data Warehouse **is** a relational database. It's just a little more than that.\n",
    "\n",
    "### Key differences:\n",
    "\n",
    "1. The Warehouse can holds data from many databases\n",
    "2. Any data stored in the Warehouse is stored for **analytics purposes only**\n",
    "3. Data within a warehouse has been processed to simplify the analysis, and avoid the need for  SQL queries that spread on 300 lines\n",
    "4. Whereas databases are optimized for extracting rows (or observations), data warehouses are optimized to have a performance boost on columns (or fields).\n",
    "\n",
    "In a nutshell: warehouses are optimized for performant analysis.\n",
    "\n",
    " **A warehouse is the perfect candidate for `LOAD` destination in ETL pipelines.**\n",
    "\n",
    "[A nice article on Alooma's blog]([https://www.alooma.com/blog/database-vs-data-warehouse](https://www.alooma.com/blog/database-vs-data-warehouse))\n",
    "\n",
    "# Cloud vendors\n",
    "\n",
    "- BigQuery, owned by Google, and part of the Google Cloud Platform\n",
    "- Redshift, owned by Amazon and part of the AWS platform\n",
    "- Snowflake\n",
    "- ...\n",
    "\n",
    "As always when choosing between different vendors, the cost structure is one the most important aspects to check. For instance, BigQuery storage is **much** cheaper than Redshift, but querying data on Redshift is **free** whereas it costs about $5/TB on BigQuery. Depending on your need, one solution might be more suitable than the other.\n",
    "\n",
    "# Amazon Redshift\n",
    "\n",
    "Redshift is the Data Warehousing solution from Amazon Web Services. As every services of the AWS family, Redshift is **Cloud-based**: you only pay for the compute and storage, and you don't have to take care of maintenance costs, or scaling the hardware to support an increasing load.\n",
    "\n",
    "### Hands-on\n",
    "\n",
    "**Reading from Redshift onto a PySpark DataFrame**\n",
    "\n",
    "    df = spark.read \\\n",
    "        .option(\"url\", \"jdbc:redshift://example.coyf2i236.eu-central-1.redshift.amazonaws.com:{PORT}/agcdb?user={USER}&password={PASSWORD\") \\\n",
    "        .option(\"dbtable\", \"table_name\") \\\n",
    "        .option(\"tempdir\", \"bucket\") \\\n",
    "        .load()\n",
    "\n",
    "**Writing to Redshift from PySpark DataFrame**\n",
    "\n",
    "    df.write \\\n",
    "        .format('com.databricks.spark.redshift') \\\n",
    "        .option('url', REDSHIFT_URI) \\\n",
    "        .option('dbtable', REDSHIFT_TABLE) \\\n",
    "        .option('aws_iam_role', REDSHIFT_IAM_ROLE) \\\n",
    "        .option('tempformat', 'csv') \\\n",
    "        .option('tempdir', REDSHIFT_TEMP_DIR) \\\n",
    "        .mode('error') \\\n",
    "        .save()\n",
    "\n",
    "As Spark uses an S3 bucket to store the intermediary files, both Spark and Redshift needs to have access to the S3 bucket.\n",
    "\n",
    "→ Ensure that the Redshift cluster has assumed an IAM role that gives it access to the `tempdir` S3 bucket (or use `forward_spark_s3_credentials` option)\n",
    "\n",
    "→ By default, Spark uses the Avro format as an intermediary storage in S3. Using CSV can significantly improve loading performance, and also allow columns to have names with characters other than ASCII letters.\n",
    "\n",
    "→ By default, every `string` column is loaded as a 256-byte length `VARCHAR` to Redshift. To gain performance or flexibility, it is possible to edit the default behavior by giving a `redshift_type` metadata to the DataFrame's column. See docs below for implementation in Scala and Python.\n",
    "\n",
    "[Amazon Redshift](https://docs.databricks.com/data/data-sources/aws/amazon-redshift.html#setting-a-custom-column-type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
