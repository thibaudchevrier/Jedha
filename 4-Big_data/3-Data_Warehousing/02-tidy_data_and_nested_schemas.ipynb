{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STRIP ###\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder \\\n",
    "         .master('local') \\\n",
    "         .appName('Tidy Data') \\\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "### END STRIP ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy data and nested schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy data\n",
    "Data tidying is the concept of structuring datasets to facilitate analysis.\n",
    "\n",
    "The principles of tidy data have been described in 2013 by statistician [Hadley Wickman](http://hadley.nz/) and closely tied to the principles of relational databases and Codd's relational algebra. They provide a standard way to organize data values within a dataset and can be synthetized as:\n",
    "\n",
    "- Each variable forms a column.\n",
    "- Each observation forms a row.\n",
    "- Each type of observational unit forms a table.\n",
    "\n",
    "We strongly advice you take the time to read [the original paper from Wickam](https://vita.had.co.nz/papers/tidy-data.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array operations and nested schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have some data about users, here we create a RDD from a dict, but in real life, we would obtain it through a pipeline or a query from a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n",
      "| id|  name|              orders|\n",
      "+---+------+--------------------+\n",
      "|  1|George|[50.61, 31.32, 20.9]|\n",
      "|  2|Hugues|[133.8, 59.0, 40....|\n",
      "+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_dct = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [50.61, 31.32, 20.9]},\n",
    "    {'id': 2, 'name': 'Hugues', 'orders': [133.8, 59.0, 40.03, 27.91]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users_dct)\n",
    "users_df = spark.createDataFrame(users_rdd.map(lambda x: Row(**x)))\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- orders: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+---+------+-----------------+\n",
      "| id|  name|           orders|\n",
      "+---+------+-----------------+\n",
      "|  1|George|     [50, 31, 20]|\n",
      "|  2|Hugues|[133, 59, 40, 27]|\n",
      "+---+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_dct = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [50, 31, 20]},\n",
    "    {'id': 2, 'name': 'Hugues', 'orders': [133, 59, 40, 27]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users_dct)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('orders', ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "users_df = spark.createDataFrame(users_rdd.map(lambda x: Row(**x)), schema=schema)\n",
    "users_df.printSchema()\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `F.size(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n",
      "| id|  name|orders_quantity|\n",
      "+---+------+---------------+\n",
      "|  1|George|              3|\n",
      "|  2|Hugues|              4|\n",
      "+---+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df \\\n",
    "    .withColumn('orders_quantity', F.size('orders')) \\\n",
    "    .drop('orders') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the size of the array, which is pretty nice, but what if we want to compute other aggregates like sum or average? It appears it's not trivial, we will go through one method but there are other, you can read more about it [here](https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `F.explode(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to compute aggregate, let's ask another question: what if we want one row per order?  \n",
    "In effect, an order is an observational unit, and following the tidy principles, deserve it's own table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- orders: double (nullable = true)\n",
      "\n",
      "+---+------+------+\n",
      "| id|  name|orders|\n",
      "+---+------+------+\n",
      "|  1|George| 50.61|\n",
      "|  1|George| 31.32|\n",
      "|  1|George|  20.9|\n",
      "|  2|Hugues| 133.8|\n",
      "|  2|Hugues|  59.0|\n",
      "|  2|Hugues| 40.03|\n",
      "|  2|Hugues| 27.91|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df = users_df.withColumn('orders', F.explode('orders'))\n",
    "orders_df.printSchema()\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we can compute the average order by customer with a `.groupBy(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------------+\n",
      "| id|  name|      avg(orders)|\n",
      "+---+------+-----------------+\n",
      "|  1|George|34.27666666666667|\n",
      "|  2|Hugues|           65.185|\n",
      "+---+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.groupBy('id', 'name') \\\n",
    "    .mean('orders') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opposite transformation is **`.collect_list(...)`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.groupBy('id', 'name') \\\n",
    "    .agg(F.collect_list('orders').alias('orders')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our original DataFrame back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested schema\n",
    "This time our schema is a bit more difficult, we have a list of users with their orders, but not only we have the order amount, we also have the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [\n",
    "        {'id': 1, 'value': 55.1},\n",
    "        {'id': 2, 'value': 78.31},\n",
    "        {'id': 4, 'value': 52.13}\n",
    "    ]},\n",
    "    {'id': 2, 'name': 'Hughes', 'orders': [\n",
    "        {'id': 3, 'value': 31.19},\n",
    "        {'id': 5, 'value': 131.1}\n",
    "    ]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('orders', ArrayType(\n",
    "        StructType([\n",
    "            StructField('id', IntegerType(), True),\n",
    "            StructField('value', FloatType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "users_df = spark.createDataFrame(users_rdd, schema=schema)\n",
    "users_df.printSchema()\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = users_df.withColumn('orders', F.explode('orders'))\n",
    "orders_df.printSchema()\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access nested fields using `.getField(fieldname)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df \\\n",
    "    .withColumn('order_id', F.col('orders').getField('id')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using **`.`** notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df \\\n",
    "    .withColumn('order_id', F.col('orders.id')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_flattened = orders_df \\\n",
    "    .withColumn('order_id', F.col('orders.id')) \\\n",
    "    .withColumn('order_value', F.col('orders.value')) \\\n",
    "    .drop('orders')\n",
    "orders_df_flattened.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_flattened \\\n",
    "    .groupBy('name') \\\n",
    "    .sum('order_value') \\\n",
    "    .orderBy('sum(order_value)') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliasing inline and descending sort\n",
    "orders_df_flattened \\\n",
    "    .groupBy('name') \\\n",
    "    .agg(F.sum('order_value').alias('total_value')) \\\n",
    "    .orderBy(F.desc('total_value')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    {'id': 1, 'name': 'George', 'orders': [\n",
    "        {'id': 1, 'items': [\n",
    "            {'id': 1, 'category': 'shirt', 'price': 80, 'quantity': 4},\n",
    "            {'id': 2, 'category': 'jeans', 'price': 130, 'quantity': 2}\n",
    "        ]},\n",
    "        {'id': 4, 'items': [\n",
    "            {'id': 1, 'category': 'shirt', 'price': 80, 'quantity': 1},\n",
    "            {'id': 3, 'category': 'shoes', 'price': 240, 'quantity': 1}\n",
    "        ]}\n",
    "    ]},\n",
    "    {'id': 2, 'name': 'Hughes', 'orders': [\n",
    "        {'id': 2, 'items': [\n",
    "            {'id': 4, 'category': 'shorts', 'price': 120, 'quantity': 3},\n",
    "            {'id': 1, 'category': 'shirt', 'price': 180, 'quantity': 2},\n",
    "            {'id': 3, 'category': 'shoes', 'prices': 240, 'quantity': 1}\n",
    "        ]},\n",
    "        {'id': 3, 'items': [\n",
    "            {'id': 5, 'category': 'suit', 'price': 2000, 'quantity': 1}\n",
    "        ]}\n",
    "    ]}\n",
    "]\n",
    "users_rdd = sc.parallelize(users)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('orders', ArrayType(\n",
    "        StructType([\n",
    "            StructField('id', IntegerType(), True),\n",
    "            StructField('items', ArrayType(\n",
    "                StructType([\n",
    "                    StructField('id', IntegerType(), True),\n",
    "                    StructField('category', StringType(), True),\n",
    "                    StructField('price', IntegerType(), True),\n",
    "                    StructField('quantity', IntegerType(), True)\n",
    "                ])\n",
    "            ))\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "users_df = spark.createDataFrame(users_rdd, schema=schema)\n",
    "users_df.printSchema()\n",
    "users_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = users_df.withColumn('orders', F.explode('orders'))\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# TODO: build this step by step and show intermediary results\n",
    "items_df = (\n",
    "    orders_df.withColumn('order_id', F.col('orders.id'))\n",
    "    .withColumn('items', F.col('orders.items'))\n",
    "    .drop('orders')\n",
    "    .withColumnRenamed('name', 'user_name')\n",
    "    .withColumnRenamed('id', 'user_id')\n",
    "    .withColumn('items', F.explode('items'))\n",
    "    .withColumn('item_id', F.col('items.id'))\n",
    "    .withColumn('item_category', F.col('items.category'))\n",
    "    .withColumn('item_price', F.col('items.price'))\n",
    "    .withColumn('item_quantity', F.col('items.quantity'))\n",
    "    .withColumn('total_price', F.col('item_price') * F.col('item_quantity'))\n",
    "    .drop('items')\n",
    ")\n",
    "items_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df \\\n",
    "    .groupBy('item_category') \\\n",
    "    .sum('item_quantity') \\\n",
    "    .orderBy(F.desc('sum(item_quantity)')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to alias, in this case, you change `.sum()` for `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df \\\n",
    "    .groupBy('item_category') \\\n",
    "    .agg(F.sum('item_quantity').alias('total_quantity')) \\\n",
    "    .orderBy(F.desc('total_quantity')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to alias.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df \\\n",
    "    .groupBy('item_category') \\\n",
    "    .agg((F.sum('total_price') / F.sum('item_quantity')).alias('avg_sale')) \\\n",
    "    .orderBy(F.desc('avg_sale')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Automatically and Elegantly flatten DataFrame in Spark SQL](https://stackoverflow.com/questions/37471346/automatically-and-elegantly-flatten-dataframe-in-spark-sql) on StackOverflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
