{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced scraping with Scrapy\n",
    "\n",
    "## What you will learn in this course 🧐🧐\n",
    "\n",
    "As you learned how to parse HTML pages, it is now time to go to the next level and scrape websites automatically. The best way to do so is by using spiders from Scrapy. In this course, we'll learn:\n",
    "\n",
    "* How to create basic crawlers \n",
    "* Target specific tags and attributes in a webpage \n",
    "* Follow links to scrap multiple pages\n",
    "* Simulate user log-in\n",
    "* Run multiple crawlers at the same time\n",
    "* Avoid being banned from websites\n",
    "\n",
    "If Scrapy isn't installed yet in your environment, just execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Scrapy\n",
      "  Using cached Scrapy-2.4.1-py2.py3-none-any.whl (239 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Using cached service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Using cached w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Using cached queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /opt/conda/lib/python3.8/site-packages (from Scrapy) (19.1.0)\n",
      "Collecting parsel>=1.5.0\n",
      "  Using cached parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Using cached zope.interface-5.2.0-cp38-cp38-manylinux2010_x86_64.whl (244 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.tar.gz (34 kB)\n",
      "Processing /home/jovyan/.cache/pip/wheels/91/64/36/bd0d11306cb22a78c7f53d603c7eb74ebb6c211703bc40b686/Protego-0.1.16-py3-none-any.whl\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Processing /home/jovyan/.cache/pip/wheels/f2/36/1b/99fe6d339e1559e421556c69ad7bc8c869145e86a756c403f4/Twisted-20.3.0-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.8/site-packages (from Scrapy) (3.1.1)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting lxml>=3.5.0; platform_python_implementation == \"CPython\"\n",
      "  Using cached lxml-4.6.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Using cached itemadapter-0.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /opt/conda/lib/python3.8/site-packages (from service-identity>=16.0.0->Scrapy) (20.2.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from w3lib>=1.17.0->Scrapy) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from zope.interface>=4.1.3->Scrapy) (49.6.0.post20201009)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting Automat>=0.3.0\n",
      "  Using cached Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Using cached incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-20.0.1-py2.py3-none-any.whl (48 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Using cached PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.8/site-packages (from cryptography>=2.0->Scrapy) (1.14.3)\n",
      "Requirement already satisfied: idna>=2.5 in /opt/conda/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->Scrapy) (2.10)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->Scrapy) (2.20)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=142f16a09634682130b6cb90ba2f14d5e4243bf4e1ae9880b27b5106e2f7acaa\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/d1/d7/61/11b5b370ee487d38b5408ecb7e0257db9107fa622412cbe2ff\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: pyasn1, pyasn1-modules, service-identity, w3lib, queuelib, cssselect, lxml, parsel, zope.interface, PyDispatcher, protego, itemadapter, jmespath, itemloaders, Automat, incremental, hyperlink, constantly, PyHamcrest, Twisted, Scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Scrapy-2.4.1 Twisted-20.3.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-20.0.1 incremental-17.5.0 itemadapter-0.2.0 itemloaders-1.0.4 jmespath-0.10.0 lxml-4.6.2 parsel-1.6.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.5.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.2.0\n"
     ]
    }
   ],
   "source": [
    "# Add '!' only if you are running this command on a notebook \n",
    "## It tells Jupyter that the command should be interpreted as bash command\n",
    "!pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first spider 🕷️🕷️\n",
    "\n",
    "Basically, Scrapy works with *Spiders* that describe the successive steps necessary to get the data you're interested in at a given url. To make a scraping engine, you will need to:\n",
    "\n",
    "- declare your own class that inherits from `Scrapy.Spider`,\n",
    "- declare two attributes: the `name` of your crawler and the `url` at which you will start crawling,\n",
    "- declare a `parse` method with an argument called `response` (which represents the variable containing the HTML response at the `url` you just defined). This method will describe all the steps required to extract the desired data from the HTML elements, by using CSS selectors.\n",
    "\n",
    "Let's begin with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os => Library used to easily manipulate operating systems\n",
    "## More info => https://docs.python.org/3/library/os.html\n",
    "import os \n",
    "\n",
    "# Import logging => Library used for logs manipulation \n",
    "## More info => https://docs.python.org/3/library/logging.html\n",
    "import logging\n",
    "\n",
    "# Import scrapy and scrapy.crawler \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomQuoteSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"randomquote\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/random',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the first <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quote = response.css('div.quote')\n",
    "        return {\n",
    "            'text': quote.css('span.text::text').get(),\n",
    "            'author': quote.css('span small.author::text').get(),\n",
    "            'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you have to declare a `CrawlerProcess` that will run the spider and save the results in a `json` file (called a \"FEED\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-12 14:04:48 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2020-12-12 14:04:48 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-12-12 14:04:48 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-12-12 14:04:48 [scrapy.extensions.telnet] INFO: Telnet Password: afe766bdc57fd3ef\n",
      "2020-12-12 14:04:48 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-12-12 14:04:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-12-12 14:04:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-12-12 14:04:48 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-12-12 14:04:48 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-12-12 14:04:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-12-12 14:04:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-12-12 14:04:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-12-12 14:04:49 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: src/1_randomquote.json\n",
      "2020-12-12 14:04:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 890,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.37466,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 12, 12, 14, 4, 49, 27294),\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 77164544,\n",
      " 'memusage/startup': 77164544,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2020, 12, 12, 14, 4, 48, 652634)}\n",
      "2020-12-12 14:04:49 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"1_randomquote.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(RandomQuoteSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: Scrapy is not made to run multiple independant crawlers in one script. For this reason, please restart your notebook's kernel before declaring a new `CrawlerProcess` (otherwise an error will be raised and the crawling won't run).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple items per page 🛍️🛍️\n",
    "\n",
    "Let's see an example where we parse multiple elements with a `for` loop and python's `yield` instruction (see appendix 1 of this lecture for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of all the <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 10:24:35 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 10:24:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2020-11-13 10:24:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:24:35 [scrapy.extensions.telnet] INFO: Telnet Password: 5b91904e46afd900\n",
      "2020-11-13 10:24:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-13 10:24:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:24:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:24:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:24:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:24:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:24:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-13 10:24:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:24:36 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/2_quotes.json\n",
      "2020-11-13 10:24:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 246,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2204,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.451172,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 24, 36, 259794),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 75149312,\n",
      " 'memusage/startup': 75149312,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 24, 35, 808622)}\n",
      "2020-11-13 10:24:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"2_quotes.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following pagination links 📄📄\n",
    "\n",
    "The example below shows how to use links to iterate over multiple pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesMultipleSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotesmultiplepages\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Select the NEXT button and store it in next_page\n",
    "            next_page = response.css('li.next a').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            # In the last page, there won't be any \"href\" and a KeyError will be raised\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            # If a next page is found, execute the parse method once again\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 10:25:11 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 10:25:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2020-11-13 10:25:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:25:11 [scrapy.extensions.telnet] INFO: Telnet Password: b090b7f78c6cb76e\n",
      "2020-11-13 10:25:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-13 10:25:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:25:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:25:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:25:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:25:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:25:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-13 10:25:14 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-11-13 10:25:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:25:14 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: src/3_quotesmultiplepages.json\n",
      "2020-11-13 10:25:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2866,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 23094,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'elapsed_time_seconds': 2.845175,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 25, 14, 573427),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75202560,\n",
      " 'memusage/startup': 75202560,\n",
      " 'request_depth_max': 9,\n",
      " 'response_received_count': 10,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 25, 11, 728252)}\n",
      "2020-11-13 10:25:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"3_quotesmultiplepages.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesMultipleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication on a website 🔐🔐\n",
    "\n",
    "A very useful feature of Scrapy: you can simulate automatic authentication!\n",
    "\n",
    "This can be done by using `scrapy.FormRequest.from_response()` to send a post request with some your login/password to the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesLogin(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"login\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['http://quotes.toscrape.com/login']\n",
    "\n",
    "    # Parse function for login\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to login\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'username': 'john', 'password': 'secret'},\n",
    "\n",
    "            # Function to be called once logged in\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_login(self, response):\n",
    "\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('li.next a').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            # In the last page, there won't be any \"href\" and a KeyError will be raised\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            # If a next page is found, execute the parse method once again\n",
    "            yield response.follow(next_page, callback=self.after_login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 10:26:29 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 10:26:29 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2020-11-13 10:26:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:26:29 [scrapy.extensions.telnet] INFO: Telnet Password: 60f7861552914475\n",
      "2020-11-13 10:26:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-13 10:26:29 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:26:29 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:26:29 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:26:29 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:26:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:26:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-13 10:26:32 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-11-13 10:26:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:26:32 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: src/4_quotesauthentication.json\n",
      "2020-11-13 10:26:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5404,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 11,\n",
      " 'downloader/request_method_count/POST': 1,\n",
      " 'downloader/response_bytes': 26193,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 3.233247,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 26, 32, 925005),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75227136,\n",
      " 'memusage/startup': 75227136,\n",
      " 'request_depth_max': 10,\n",
      " 'response_received_count': 11,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 26, 29, 691758)}\n",
      "2020-11-13 10:26:32 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"4_quotesauthentication.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesLogin)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple spiders simultaneously 🕸️ 🕷️\n",
    "\n",
    "As stated before, it's not possible to run multiple crawlers in the same python script. But if you'd like to crawl different pages in parallel, this can be done by declaring multiple spiders!\n",
    "\n",
    "Then you just have to call `process.crawl()` as many times as you need, by passing your different spiders, as we illustrate below. The results will all be stored as a list of JSON data in the same file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpiderPage1(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of all <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "            \n",
    "\n",
    "class QuotesSpiderPage2(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        quotes = response.css('div.quote')\n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 10:27:54 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 10:27:54 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2020-11-13 10:27:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:27:54 [scrapy.extensions.telnet] INFO: Telnet Password: 226a569a2ba7e62a\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:27:54 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:27:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:27:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-13 10:27:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:27:54 [scrapy.extensions.telnet] INFO: Telnet Password: fd01a6b00553a551\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:27:54 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:27:54 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:27:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:27:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2020-11-13 10:27:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:27:55 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/5_quotesmultiplespiders.json\n",
      "2020-11-13 10:27:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 246,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 3096,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.476214,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 27, 55, 186234),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 75476992,\n",
      " 'memusage/startup': 75476992,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 27, 54, 710020)}\n",
      "2020-11-13 10:27:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2020-11-13 10:27:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:27:55 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/5_quotesmultiplespiders.json\n",
      "2020-11-13 10:27:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 246,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2200,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.530724,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 27, 55, 192626),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 24,\n",
      " 'memusage/max': 75288576,\n",
      " 'memusage/startup': 75288576,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 27, 54, 661902)}\n",
      "2020-11-13 10:27:55 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"5_quotesmultiplespiders.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesSpiderPage1)\n",
    "process.crawl(QuotesSpiderPage2)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid being banned: autothrottle 🚫🚫\n",
    "\n",
    "The more scraping you're doing the more requests you make. If websites are well protected, they might ban you because you exceeded requests limitations. \n",
    "\n",
    "You may avoid that by delaying the number of requests automatically thanks to the `AutoThrottle` extension. \n",
    "\n",
    "As stated in the documentation, `AutoThrottle` extension is designed to: \n",
    "\n",
    "- *Be nicer to sites instead of using default download delay of zero.*\n",
    "- *Automatically adjust Scrapy to the optimum crawling speed, so the user doesn’t have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest.*\n",
    "\n",
    "To use autothrottle, it's as simple as adding `\"AUTOTHROTTLE_ENABLED\": True` to your crawler's settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesMultipleSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotesmultiplepages\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback that gets text, author and tags of the webpage\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.css('li.next a').attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            # In the last page, there won't be any \"href\" and a KeyError will be raised\n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        else:\n",
    "            # If a next page is found, execute the parse method once again\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-13 10:31:19 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-11-13 10:31:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Sep  4 2020, 02:22:02) - [Clang 10.0.0 ], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2020-11-13 10:31:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-11-13 10:31:19 [scrapy.extensions.telnet] INFO: Telnet Password: 86b452237fed67cf\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-11-13 10:31:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-11-13 10:31:19 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-11-13 10:31:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-11-13 10:31:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-11-13 10:31:32 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-11-13 10:31:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-11-13 10:31:32 [scrapy.extensions.feedexport] INFO: Stored json feed (100 items) in: src/6_quotesautothrottle.json\n",
      "2020-11-13 10:31:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2866,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 23094,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 10,\n",
      " 'elapsed_time_seconds': 12.883837,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 11, 13, 9, 31, 32, 254774),\n",
      " 'item_scraped_count': 100,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75186176,\n",
      " 'memusage/startup': 75186176,\n",
      " 'request_depth_max': 9,\n",
      " 'response_received_count': 10,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2020, 11, 13, 9, 31, 19, 370937)}\n",
      "2020-11-13 10:31:32 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Name of the file where the results will be saved\n",
    "filename = \"6_quotesautothrottle.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename: {\"format\": \"json\"},\n",
    "    },\n",
    "    \"AUTOTHROTTLE_ENABLED\": True  # AutoThrottle Here!\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(QuotesMultipleSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1 - What is Yield keyword for? 💐\n",
    "\n",
    "You might have noticed that we used the `yield` keyword in Scrapy which could be quite new and confusing. Technically speaking it is called a *generator*.\n",
    "\n",
    "In a nutshell, `yield` is a very useful keyword to return a data collection without taking up too much machine's memory. \n",
    "\n",
    "Let's check out with an example. Let's take two functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function with return keyword\n",
    "def return_list(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        a_list[i] *= 2\n",
    "    return a_list\n",
    "\n",
    "# Function with yield keyword\n",
    "def return_with_yield(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        yield a_list[i] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply these two functions to our `random_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numbers from 0 to 9\n",
    "random_list = [x for x in range(10)]\n",
    "# Returns a list\n",
    "return_list(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object return_with_yield at 0x10a14a190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numbers from 0 to 9\n",
    "random_list = [x for x in range(10)]\n",
    "# Function with yield\n",
    "return_with_yield(random_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example, `return_list` returned directly the full list. Whereas, in the second example, `return_with_yield` returned a `generator`. Generators are very cool because we haven't actually executed the loop. Therefore, we haven't spend too much computer memory. \n",
    "\n",
    "So let's say instead of a list of 10 items, you'd have one of 1000000 items, it would make a huge difference in terms of computing time. \n",
    "\n",
    "Now if you need to get the actual values of your generator, you can simply create a for loop or a comprehension list like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 0\n",
      "output 2\n",
      "output 4\n",
      "output 6\n",
      "output 8\n",
      "output 10\n",
      "output 12\n",
      "output 14\n",
      "output 16\n",
      "output 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a for loop will just print the output:\n",
    "for number in return_with_yield(random_list):\n",
    "    print(\"output\", number)\n",
    "\n",
    "# Using a comprehension list will create a list:\n",
    "[i for i in return_with_yield(random_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you simply need to yield from a list without doing any manipulation, you can use `yield from` instead of creating a loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2 - Crash course on XPath ⚔️\n",
    "\n",
    "In this lecture, you've learned how to use CSS selectors with Scrapy. Another way of scraping websites with Scrapy is by using XPaths.\n",
    "\n",
    "The best way to learn XPath is to follow this great tutorial from <a href=\"http://zvon.org/comp/r/tut-XPath_1.html#Pages~List_of_XPaths\" target=\"_blank\">http://Zvon.org</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources 📚📚\n",
    "\n",
    "* <a href=\"https://docs.scrapy.org/en/latest/index.html\" target=\"_blank\"> Scrapy Documentation </a>\n",
    "* <a href=\"https://docs.python.org/3/library/logging.html\" target=\"_blank\"> Logging</a>\n",
    "* <a href=\"https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging\" target=\"_blank\">Logging in a scrapy</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
