{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping - Scrapy \n",
    "\n",
    "## What you'll learn in this course \n",
    "\n",
    "As you learned how to parse HTML pages, it is now time to go to the next level and scrape websites automatically. The best way to do so is by using spiders from Scrapy. In this course, we'll learn: \n",
    "\n",
    "* How to create basic crawlers \n",
    "* Target specific tags and attributes in a webpage \n",
    "* Follow links \n",
    "* Simulate user log-in\n",
    "* Create Item Pipelines \n",
    "* Deploy Scrapy to scrapinghub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Scrapy 🚧🚧\n",
    "\n",
    "Scrapy is not usually pre-installed in your environment. We strongly advise you to install it in a virtual environment. To do so, make sure you have `virtualenv` install. Otherwise: \n",
    "\n",
    "```shell\n",
    "pip install virtualenv\n",
    "```\n",
    "\n",
    "then create a directory: \n",
    "\n",
    "```shell\n",
    "mkdir scrapy\n",
    "```\n",
    "```shell\n",
    "cd scrapy\n",
    "```\n",
    "\n",
    "then create a virtual environment within this directory: \n",
    "\n",
    "```shell\n",
    "virtualenv env\n",
    "```\n",
    "```shell\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "(NB: You should see a `(env)` in your terminal next to your username if everything worked well)\n",
    "\n",
    "You can now run:\n",
    "\n",
    "```shell\n",
    "pip install Scrapy\n",
    "```\n",
    "\n",
    "This install `scrapy` within your virtual environment. Once you will be done playing around with the tool, you can do: \n",
    "\n",
    "```shell \n",
    "deactivate\n",
    "```\n",
    "\n",
    "This will deactivate your environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore with Scrapy shell 🔍\n",
    "\n",
    "Let's take the website that scrapy uses in its documentation: http://quotes.toscrape.com/\n",
    "To help you select the right element, you can use scrapy shell:   \n",
    "```shell\n",
    "scrapy shell \"https://quotes.toscrape.com\"\n",
    "```\n",
    "You can start by typing `view(response)` to visualize the website you'll be scraping. It should open an HTML page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select elements from a webpage \n",
    "\n",
    "If you want to select elements from a webpage, you can do it in two ways: \n",
    "\n",
    "1. With CSS selectors \n",
    "2. With XPaths \n",
    "\n",
    "CSS selectors are easier to use eventhough scrapy recommends you to use XPath (We've provided a crash course on XPath in the appendix of the course.)\n",
    "\n",
    "Let's select all the quotes from our page, we can simply do it by typing: \n",
    "\n",
    "```shell \n",
    "response.css('.text').getall()\n",
    "```\n",
    "\n",
    "As you can see, we selected the class `text` that corresponds to the HTML tag where the quotes are. Then we called the `.getall()` method to get all the elements that contains the class `text` in the webpage. \n",
    "\n",
    "The output should look like this: \n",
    "\n",
    "```shell \n",
    "['<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>', '<span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>', '<span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>', '<span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>', '<span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it\\'s better to be absolutely ridiculous than absolutely boring.”</span>', '<span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>', '<span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>', '<span class=\"text\" itemprop=\"text\">“I have not failed. I\\'ve just found 10,000 ways that won\\'t work.”</span>', '<span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it\\'s in hot water.”</span>', '<span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to get the HTML tag, but simply the text, you can specify it in your `.css()` method. \n",
    "\n",
    "```shell\n",
    "response.css('.text::text').getall()\n",
    "```\n",
    "\n",
    "The output should look like this: \n",
    "\n",
    "```shell \n",
    "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", '“Try not to become a man of success. Rather become a man of value.”', '“It is better to be hated for what you are than to be loved for what you are not.”', \"“I have not failed. I've just found 10,000 ways that won't work.”\", \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", '“A day without sunshine is like, you know, night.”']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent in XPath would be: \n",
    "\n",
    "```shell \n",
    "response.xpath('//span/text()').getall()\n",
    "```\n",
    "\n",
    "The XPath is basically looking for all the spans in the HTML page and get the text of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select an attribute of an HTML tag \n",
    "\n",
    "Sometimes, you don't want to have the text contained in an HTML tag but rather its attributes. For example, you might want to get the url of a link. You can do it by using the `.attrib` attribute. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell \n",
    "response.css(\"small+a\").attrib['href']\n",
    "```\n",
    "\n",
    "You should get the following output:\n",
    "\n",
    "```\n",
    "'/author/Albert-Einstein'\n",
    "```\n",
    "\n",
    "If you want you to get the full url, you can do: \n",
    "\n",
    "```shell \n",
    "response.urljoin(response.css(\"small+a\").attrib[\"href\"])\n",
    "```\n",
    "\n",
    "You would get the following output \n",
    "\n",
    "```shell \n",
    "'http://quotes.toscrape.com/author/Albert-Einstein'\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first project 💼💼\n",
    "\n",
    "To create a project with Scrapy, go to your terminal and simply run: \n",
    "\n",
    "```shell \n",
    "scrapy startproject my_first_project\n",
    "```\n",
    "\n",
    "You should see the following output: \n",
    "\n",
    "```shell \n",
    "New Scrapy project 'my_first_project', using template directory '/Users/antoinekrajnc/Desktop/test_scrapy/virt/lib/python3.7/site-packages/scrapy/templates/project', created in:\n",
    "    /Users/antoinekrajnc/Desktop/test_scrapy/my_first_project\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd my_first_project\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "\n",
    "`cd` into your project and you should see the following tree: \n",
    "\n",
    "```shell \n",
    ".\n",
    "├── __init__.py\n",
    "├── __pycache__\n",
    "├── items.py\n",
    "├── middlewares.py\n",
    "├── pipelines.py\n",
    "├── settings.py\n",
    "└── spiders\n",
    "    ├── __init__.py\n",
    "    └── __pycache__\n",
    "\n",
    "```\n",
    "\n",
    "We will learn the purpose of the main files as we progress in this course. For the moment, we'll simply be working inside the `spiders` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Spiders 🕷️🕷️\n",
    "\n",
    "Spiders are the crawlers that will actually scrape data from a website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example \n",
    "\n",
    "Run \n",
    "```shell\n",
    "scrapy genspider my_first_spider \"https://quotes.toscrape.com\"\n",
    "```\n",
    "It creates a new file `my_first_spider.py` within the `spiders` folder: \n",
    "\n",
    "```shell\n",
    "├── __init__.py\n",
    "├── items.py\n",
    "├── middlewares.py\n",
    "├── pipelines.py\n",
    "├── settings.py\n",
    "└── spiders\n",
    "    ├── __init__.py\n",
    "    └── my_first_spider.py\n",
    "```\n",
    "\n",
    "You can modify `my_first_spider.py` with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback that gets text, author and tags of the webpage\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        # Select the NEXT button and store it in next_page\n",
    "        next_page = response.css('li.next a').attrib[\"href\"]\n",
    "\n",
    "        # Check if next_page exists\n",
    "        if next_page is not None:\n",
    "            # Follow the next page and use the callback parse\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Spider \n",
    "\n",
    "Now if you want to run your spider, the simplest way is to run: \n",
    "\n",
    "```shell \n",
    "scrapy crawl quotes -o quotes.json\n",
    "```\n",
    "\n",
    "After the command ended, you should see a file called: `quotes.json` that contains: \n",
    "\n",
    "```json \n",
    "[\n",
    "{\"text\": \"\\u201cThis life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.\\u201d\", \"author\": \"Marilyn Monroe\", \"tags\": [\"friends\", \"heartbreak\", \"inspirational\", \"life\", \"love\", \"sisters\"]},\n",
    "{\"text\": \"\\u201cIt takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.\\u201d\", \"author\": \"J.K. Rowling\", \"tags\": [\"courage\", \"friends\"]},\n",
    "{\"text\": \"\\u201cIf you can't explain it to a six year old, you don't understand it yourself.\\u201d\", \"author\": \"Albert Einstein\", \"tags\": [\"simplicity\", \"understand\"]},\n",
    "{\"text\": \"\\u201cYou may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect\\u2014you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break\\u2014her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.\\u201d\", \"author\": \"Bob Marley\", \"tags\": [\"love\"]},\n",
    "{\"text\": \"\\u201cI like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.\\u201d\", \"author\": \"Dr. Seuss\", \"tags\": [\"fantasy\"]},\n",
    "{\"text\": \"\\u201cI may not have gone where I intended to go, but I think I have ended up where I needed to be.\\u201d\", \"author\": \"Douglas Adams\", \"tags\": [\"life\", \"navigation\"]},\n",
    "{\"text\": \"\\u201cThe opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.\\u201d\", \"author\": \"Elie Wiesel\", \"tags\": [\"activism\", \"apathy\", \"hate\", \"indifference\", \"inspirational\", \"love\", \"opposite\", \"philosophy\"]},\n",
    "{\"text\": \"\\u201cIt is not a lack of love, but a lack of friendship that makes unhappy marriages.\\u201d\", \"author\": \"Friedrich Nietzsche\", \"tags\": [\"friendship\", \"lack-of-friendship\", \"lack-of-love\", \"love\", \"marriage\", \"unhappy-marriage\"]},\n",
    "{\"text\": \"\\u201cGood friends, good books, and a sleepy conscience: this is the ideal life.\\u201d\", \"author\": \"Mark Twain\", \"tags\": [\"books\", \"contentment\", \"friends\", \"friendship\", \"life\"]},\n",
    "{\"text\": \"\\u201cLife is what happens to us while we are making other plans.\\u201d\", \"author\": \"Allen Saunders\", \"tags\": [\"fate\", \"life\", \"misattributed-john-lennon\", \"planning\", \"plans\"]},\n",
    "{\"text\": \"\\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\\u201d\", \"author\": \"Albert Einstein\", \"tags\": [\"change\", \"deep-thoughts\", \"thinking\", \"world\"]},\n",
    "{\"text\": \"\\u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.\\u201d\", \"author\": \"J.K. Rowling\", \"tags\": [\"abilities\", \"choices\"]},\n",
    "{\"text\": \"\\u201cThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.\\u201d\", \"author\": \"Albert Einstein\", \"tags\": [\"inspirational\", \"life\", \"live\", \"miracle\", \"miracles\"]},\n",
    "{\"text\": \"\\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\\u201d\", \"author\": \"Jane Austen\", \"tags\": [\"aliteracy\", \"books\", \"classic\", \"humor\"]},\n",
    "{\"text\": \"\\u201cImperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.\\u201d\", \"author\": \"Marilyn Monroe\", \"tags\": [\"be-yourself\", \"inspirational\"]},\n",
    "{\"text\": \"\\u201cTry not to become a man of success. Rather become a man of value.\\u201d\", \"author\": \"Albert Einstein\", \"tags\": [\"adulthood\", \"success\", \"value\"]},\n",
    "{\"text\": \"\\u201cIt is better to be hated for what you are than to be loved for what you are not.\\u201d\", \"author\": \"Andr\\u00e9 Gide\", \"tags\": [\"life\", \"love\"]},\n",
    "{\"text\": \"\\u201cI have not failed. I've just found 10,000 ways that won't work.\\u201d\", \"author\": \"Thomas A. Edison\", \"tags\": [\"edison\", \"failure\", \"inspirational\", \"paraphrased\"]},\n",
    "{\"text\": \"\\u201cA woman is like a tea bag; you never know how strong it is until it's in hot water.\\u201d\", \"author\": \"Eleanor Roosevelt\", \"tags\": [\"misattributed-eleanor-roosevelt\"]},\n",
    "{\"text\": \"\\u201cA day without sunshine is like, you know, night.\\u201d\", \"author\": \"Steve Martin\", \"tags\": [\"humor\", \"obvious\", \"simile\"]}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here, pay attention to the different elements in this command: \n",
    "\n",
    "* `scrapy crawl` tells scrapy to start running a spider \n",
    "\n",
    "* `quotes` is the name of the spider we specified when building our class \n",
    "\n",
    "* `-o quotes.json` stands for output in a file called `quotes.json`\n",
    "\n",
    "\n",
    "And that's it!🤩 You have scraped data through the web. Now let's check out more complex features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down this code a little bit:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback that gets text, author and tags of the webpage\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now several things to understand in this code: \n",
    "\n",
    "* `QuotesSpider(scrapy.Spider)`: We create a class that inherites from `scrapy.Spider`. This allow us to use all the methods and attributes from it and therefore be able to create our spider. \n",
    "\n",
    "* `name = quotes`: This is the name of your spider \n",
    "\n",
    "* `start_urls`: Must be a list of urls. Indeed `scrapy` needs to receive an iterable here. \n",
    "\n",
    "* `parse`: function is what we call a [`callback`](https://stackoverflow.com/questions/1319074/parallel-python-what-is-a-callback) this a function that will actually do the job of getting the data. \n",
    "\n",
    "* `yield`: it's a keyword well used when one needs to iterate over very long loops. Please check out our appendix if you want to learn more. \n",
    "\n",
    "In this code, we will be creating dictionnaries that will store: \n",
    "\n",
    "* The text of a quote \n",
    "* Its author \n",
    "* The tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following links \n",
    "\n",
    "Very often, you want all the pages of a website. In this case, you won't be able to hard code each page you want your crawler to run to. You will need to follow links. Here is how you'd do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of code should be familiar by now. The only thing new is the following: \n",
    "\n",
    "```python \n",
    "        # Select the NEXT button and store it in next_page\n",
    "        next_page = response.css('li.next a').attrib[\"href\"]\n",
    "\n",
    "        # Check if next_page exists\n",
    "        if next_page is not None:\n",
    "            # Follow the next page and use the callback parse\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `response.css('li.next a').attrib[\"href\"]` gets the link to the next page of the website \n",
    "\n",
    "* `if next_page is not None:` means that we are going to execute the code only if there is another page that exists\n",
    "\n",
    "* `yield response.follow(next_page, callback=self.parse)` means that we are following the link that is inside `next_page` and once the spider is on that page, it's going to execute the function that is inside the `callback` argument, i.e `parse`. It will trigger a recursive function that will be executed as many times as there are webpages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now even shorten that code by using: `.follow_all` combined with `yield from`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "\n",
    "    # Name of your spider\n",
    "    name = \"quotes\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    # Callback that gets text, author and tags of the webpage\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        yield from response.follow_all(css=\"li.next a\", callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate user login \n",
    "\n",
    "Very often, you will need to log into a given website before being able to scrape it. The best way to do so is to use `FormRequest.from_response` which will take care of login. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "\n",
    "class Login(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"login\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['http://quotes.toscrape.com/login']\n",
    "\n",
    "    # Parse function for login\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to login\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'username': 'john', 'password': 'secret'},\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_login(self, response):\n",
    "\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        yield from response.follow_all(css=\"li.next a\", callback=self.after_login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoThrottle extension\n",
    "\n",
    "As you are becoming a pro in webscraping. It might happen that you have a lot of requests to make. If websites are well protected, they might ban you from making any more requests. \n",
    "\n",
    "One way to avoid that is to delay the number of requests automatically by using the AutoThrottle extension. \n",
    "\n",
    "As the documentation states, AutoThrottle extension is designed to: \n",
    "\n",
    "* *be nicer to sites instead of using default download delay of zero*\n",
    "\n",
    "* *automatically adjust Scrapy to the optimum crawling speed, so the user doesn’t have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be able to use the AutoThrottle Extension, you will need to edit your `settings.py` within your project. \n",
    "\n",
    "```shell \n",
    "├── __init__.py\n",
    "├── items.py\n",
    "├── middlewares.py\n",
    "├── pipelines.py\n",
    "├── quotes.json\n",
    "├── settings.py\n",
    "└── spiders\n",
    "    ├── __init__.py\n",
    "    ├── login.py\n",
    "    └── my_first_spider.py\n",
    "```\n",
    "\n",
    "Especially, you will need to edit: \n",
    "\n",
    "* `AUTOTHROTTLE_ENABLED`\n",
    "\n",
    "* `AUTOTHROTTLE_START_DELAY`\n",
    "\n",
    "* `AUTOTHROTTLE_MAX_DELAY`\n",
    "\n",
    "* `AUTOTHROTTLE_TARGET_CONCURRENCY`\n",
    "\n",
    "* `AUTOTHROTTLE_DEBUG`\n",
    "\n",
    "* `CONCURRENT_REQUESTS_PER_DOMAIN`\n",
    "\n",
    "* `CONCURRENT_REQUESTS_PER_IP`\n",
    "\n",
    "* `DOWNLOAD_DELAY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your `settings.py`, simply uncomment the above variables and it autoThrottle extension will be enabled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Exports \n",
    "\n",
    "One very important feature that is and extremely useful is to have control on how and where your output data is going to be stored. To enable this, you will handle what scrapy calls `FEEDS`\n",
    "\n",
    "In `settings.py`, add another paramater called `FEEDS`. It should follow the following structure: \n",
    "\n",
    "```python \n",
    "FEEDS = {\n",
    "    \"URI_or_FILE\": {\n",
    "        \"format\": \"format_of_file\",\n",
    "        \"encoding\":\"encoding_format\",\n",
    "        \"fields\": \"fields_you_want_to_export\",\n",
    "        \"indent\": \"indentation_space_number\",\n",
    "        \"store_empty\": \"boolean_to_export_empty_feeds\"\n",
    "        \n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "For example, you can have: \n",
    "\n",
    "```python \n",
    "FEEDS = {\n",
    "    \"items.json\": {\n",
    "        \n",
    "        'format': 'json',\n",
    "        'encoding': 'utf8',\n",
    "        'store_empty': False,\n",
    "        'fields': None,\n",
    "        'indent': 4,\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Once enabled, each time you run a spider without specifying an output, it will export it there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy your Spiders \n",
    "\n",
    "Finally, to deploy your spiders, there is nothing simpler. You will need to : \n",
    "\n",
    "1. Create an account on https://app.scrapinghub.com/\n",
    "\n",
    "2. Create a project in your account \n",
    "\n",
    "2. install shub --> `pip install shub`\n",
    "\n",
    "3. run `shub login` and provide the api key that has been given when you created a repository in your account. \n",
    "\n",
    "4. run `shub deploy project_name` \n",
    "\n",
    "5. That's it!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now run spiders directly from your account or simply by api call. Check out the documentation for more information. \n",
    "\n",
    "https://doc.scrapinghub.com/scrapy-cloud.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1 - What is Yield Keyword for?\n",
    "\n",
    "You might have noticed that we used `yield` keyword in Scrapy which could be quite new and confusing. \n",
    "\n",
    "In a nutshell, `yield` is a very useful keyword to return a series of data without taking up too much machine's memory. \n",
    "\n",
    "Let's check out with an example. Let's take two functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function with return keyword\n",
    "def return_list(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        a_list[i] *= 2\n",
    "    return a_list\n",
    "\n",
    "# Function with yield keyword\n",
    "def return_with_yield(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        yield a_list[i] * 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply these two functions to a `range_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_list = [x for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list\n",
    "return_list(range_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object return_with_yield at 0x7fd5dc411a98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function with yield\n",
    "return_with_yield(range_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the full list is returned in the first example whereas the second example returned a `generator`. Generators are very cool because we haven't actually executed the loop. Therefore, we haven't spend too much computer memory. \n",
    "\n",
    "So let's say instead of a list of 10 items, you'd have 1000000 items, it would make a huge difference in terms of computing memory. \n",
    "\n",
    "Now if you need to get the actual values of your generator, you can simply create a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in return_with_yield(range_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you simply need to yield from a list without doing any manipulation, you can use `yield from` instead of creating a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yield from\n",
    "def return_with_yield(a_list):\n",
    "    yield from a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate through the generator \n",
    "[i * 2 for i in return_with_yield(range_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2 - Crash course on XPath \n",
    "\n",
    "Eventhough we can use CSS selectors, it could be good for you learn XPaths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to learn XPath is to follow this great tutorial from https://Zvon.org :\n",
    "\n",
    "* Start here : http://zvon.org/comp/r/tut-XPath_1.html#Pages~List_of_XPaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "* [FormRequest.from_response()](https://docs.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-request-userlogin)\n",
    "* [Downloading and processing files](https://docs.scrapy.org/en/latest/topics/media-pipeline.html)\n",
    "* [Avoid Getting Banned](https://docs.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned)\n",
    "* [What's YIELD in Python](https://www.youtube.com/watch?v=akqjaqUzdnA)\n",
    "* [When to use yields in python](https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/#:~:text=When%20to%20use%20yield%20instead%20of%20return%20in%20Python%3F,where%20it%20is%20left%20off.&text=If%20the%20body%20of%20a,automatically%20becomes%20a%20generator%20function.)\n",
    "* [In practice, what are the main uses for the new “yield from” syntax in Python 3.3?\n",
    "](https://stackoverflow.com/questions/9708902/in-practice-what-are-the-main-uses-for-the-new-yield-from-syntax-in-python-3)\n",
    "* [AutoThrottle Extension](https://docs.scrapy.org/en/latest/topics/autothrottle.html)\n",
    "* [Feed Exports](https://docs.scrapy.org/en/latest/topics/feed-exports.html)\n",
    "* [Scrapinghub Reference Documentation](https://doc.scrapinghub.com/scrapy-cloud.html?_ga=2.217147093.2103382170.1591108132-2059253340.1591108132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
