{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning - Linear regression\n",
    "\n",
    "<!-- TOC START min:2 max:4 link:true asterisk:true update:true -->\n",
    "* [Introduction](#introduction)\n",
    "  * [What is the supervised machine learning?](#what-is-the-supervised-machine-learning-)\n",
    "  * [Why do machine learning?](#why-do-machine-learning-)\n",
    "* [Revision linear regressions](#revision-linear-regressions)\n",
    "  * [Simple linear regressions](#simple-linear-regressions)\n",
    "    * [Definition](#definition)\n",
    "    * [Assumptions behind linear regression](#assumptions-behind-linear-regression)\n",
    "    * [Estimate](#estimate)\n",
    "  * [Multiple linear regression](#multiple-linear-regression)\n",
    "    * [Definition](#définition-3)\n",
    "    * [Assumptions behind a multiple linear regression](#assumptions-behind-a-multiple-linear-regression)\n",
    "    * [Final remarks](#final-remarks)\n",
    "<!-- TOC END -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What is the supervised machine learning?\n",
    "\n",
    "Machine learning is a multidisciplinary science that aims to enable a machine to solve complex problems that cannot be solved by simple algorithms. For example, predicting the conversion rate on a website or the price of real estate are problems that can only be solved by a set of rules.\n",
    "\n",
    "Supervised machine learning** is a branch of this discipline which aims to solve problems for which there are already solved examples. For example, data is being collected on a sample of housing units in San Francisco that describe their location, various characteristics and the amount of rent. If our problem is to estimate the rent for a dwelling in the same city that is not in our database, we will construct a model from our data that estimates the rent based on the characteristics of the dwelling and apply this model to the unknown dwelling to estimate the rent. This is a supervised learning problem, because when the model was constructed, the values of the **variable** to be estimated were known, the so-called **target variable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do machine learning?\n",
    "\n",
    "Once a problem has been posed, a target variable selected, and a number of explanatory variables collected, the objectives of supervised statistical modelling fall into three broad, non-exclusive categories:\n",
    "\n",
    "\n",
    "* Description: we can try to understand the relations that can exist between the target variable \"Y\" and the explanatory variables \"X1, ..., Xp\" in order to select the most relevant one or to obtain a visualization of the behaviors in the observed population (be careful here, population is used as a statistical term and can designate persons, countries, financial transactions, etc...).\n",
    "* Explanation: when one has a knowledge of the subject matter, as is often the case in economics or biology for example, the objective is to build a test that allows to verify or confirm theoretical results in practical situations.\n",
    "* Prediction: here we focus on the quality of estimators and predictors, we do not necessarily try to model the observed population as well as possible, but to build a model that allows us to produce reliable predictions for future observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision linear regressions\n",
    "\n",
    "In this section we will focus on regression models, which bring together the solutions that allow us to estimate a numerical and **continuous** target variable. Of particular interest will be the estimation of real estate prices in Boston, using a dataset provided in the scikit learn package from python.\n",
    "\n",
    "\n",
    "### Simple linear regressions\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Simple linear regression models are based on the following linear equation :\n",
    "\n",
    "## $E(Y) = f(X) = \\beta_{0} + \\beta_{1}X$\n",
    "\n",
    "#### OR\n",
    "\n",
    "## $Y = \\beta_{0} + \\beta_{1}X + \\epsilon$\n",
    "\n",
    "\n",
    "\n",
    "Here, $Y$ represents the target variable, i.e. the variable whose value we wish to estimate, $X$ is the explanatory variable we have chosen to estimate the target variable. $\\beta 0$, $\\beta_{1}$, $\\epsilon$ are respectively the intercept (i.e. level 0 of $Y$ when $X$ is 0), the coefficient associated with $X$ (it is the parameter of the model that measures the influence of $X$ on $Y$, if $X$ increases by 1, $Y$ will increase by $\\beta_{1}$), and the error or residue of the model. Indeed, the above equation is the representation of a statistical model: it is true on average but does not claim to be exact, which explains the presence of the residual.\n",
    "\n",
    "Depending on the individuals (or point cloud), your model will find the line that comes as close as possible to all the individuals at once. Here's what it looks like visually:\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1TO3HA0zSs3O2AtNmJSpwuNtqkh5ASGOi)\n",
    "\n",
    "\n",
    "\n",
    "##### Dependent variables\n",
    "\n",
    "In Machine Learning, we always distinguish between **dependent variables/target variables** and **independent variables/explanatory variables.** Dependent variables are the elements you are trying to predict. In the equation above, this corresponds to $Y$.\n",
    "\n",
    "\n",
    "\n",
    "##### Independent Variables\n",
    "\n",
    "The independent variables, represented by $X$ are your predictors or factors that will determine the value of $Y$. For example, if we try to predict someone's salary based on their years of experience. The independent variable $X$ corresponds to the number of years of experience.\n",
    "\n",
    "\n",
    "\n",
    "##### Coefficient\n",
    "\n",
    "The $\\beta_{1}$ coefficient represents the _slope_ or weight your independent variable will have in your equation.\n",
    "\n",
    "\n",
    "\n",
    "##### Constant\n",
    "\n",
    "Finally, the constant $\\beta_{0}$ represents where your line will start if $X = 0$. In the case of predicting wages in relation to years of experience, even at 0 years of experience ($X = 0$), the starting minimum wage is different from 0.\n",
    "\n",
    "\n",
    "\n",
    "##### Residual\n",
    "\n",
    "The residue, often noted $\\epsilon$ corresponds to the error committed during the modeling. This error corresponds to all the information that is not explained by the model. It is often assumed that the error follows a particular law of probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions behind linear regression\n",
    "\n",
    "When building an ML model, you should be aware of the assumptions you need to respect in order for your model to work well. Otherwise, you will have a poor performance. The assumptions of a simple linear regression model are as follows:\n",
    "\n",
    "\n",
    "### Linearity\n",
    "\n",
    "The first assumption is simple. You need your points to follow roughly a straight line. In other words, you need to ensure that your dependent variable grows linearly as your independent variables increase.\n",
    "\n",
    "\n",
    "\n",
    "### Homoscedasticity\n",
    "\n",
    "Beyond the complexity of the model itself, this means that the variance of your points must be relatively the same. If you have a huge variance, it means that you have points far apart and it will be difficult to get a line that is representative of your dataset.\n",
    "\n",
    "\n",
    "\n",
    "### Normality of variables\n",
    "\n",
    "The points should have a normal (or at least approximately normal) distribution, which is rarely the case. The trick is to get close to it with a mean, median and mode that is not too far away.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate\n",
    "\n",
    "### Maximizing Likelihood\n",
    "\n",
    "#### Definition\n",
    "\n",
    "The likelihood of a family of random variables is a function that gives for each possible realisation of each $β1$ and $x_{1}$ random variable in the family the probability that this combination of realisations will occur. In the case of statistical modelling, data are already available, so we already know the realizations of each random variable (i.e. the value of the explanatory variables for each observation). It is therefore a matter of finding the parameters that will make the observations available to us as likely as possible.  \n",
    "\n",
    "Statistical likelihood is a function of conditional probabilities (it is a probability whose law depends on parameters). Let $X = (x_{1}, x_{2}, ..., x_{n})$, a vector of random variables and $\\theta = (\\theta_{1}, \\theta_{2}, ..., \\theta_{k})$ of the set of parameters on which $X$ depends. The verisimilitude of $X$ is written:\n",
    "\n",
    "\n",
    "### $L(x_{1},x_{1},...x_{1}, = \\prod^{n}_{i=1}f(x_{i};\\theta_{1}, \\theta_{2}, ..., \\theta_{k})$\n",
    "\n",
    "\n",
    "\n",
    "With\n",
    "\n",
    "if x is a continuous variable:\n",
    "\n",
    "### $f(x_{i};\\Theta) = f_{\\Theta}(x_{i})$\n",
    "\n",
    "if x is a discret variable:\n",
    "\n",
    "### $f(x_{i};\\Theta) = P_{\\Theta}(x_{i})$\n",
    "\n",
    "\n",
    "\n",
    "### Maximum likelihood estimate\n",
    "\n",
    "One way to estimate the parameters of a model is to maximize the corresponding likelihood function. In the case of simple linear regression, this function is:\n",
    "\n",
    "### $L(Y = y|\\Theta) = \\prod_{i-1}^{n}f(Y_{i} = y_{1};\\Theta$\n",
    "\n",
    "### $L(Y = y|\\Theta) = \\prod_{i-1}^{n}f(\\beta_{0} ++ \\epsilon = y_{0}|\\beta_{0}, \\beta_{1})$\n",
    "\n",
    "### $L(Y = y|\\Theta) = \\prod_{i-1}^{n}f(\\epsilon = y_{i}-\\beta_{0}-\\beta_{1}x_{i}|\\beta_{0}, \\beta_{1}$\n",
    "\n",
    "### $L(Y = y|\\Theta) = \\prod_{i-1}^{n}\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(\\frac{-(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2}{2\\sigma^2})$\n",
    "\n",
    "\n",
    "This likelihood equation can be obtained by assuming that the error $\\epsilon$ follows a centred normal law ($E(\\epsilon) = \\mu = 0$) of standard deviation $\\sigma$. We must therefore find the maximum (if any) of this likelihood function. To do this we apply a logarithm to each side of the equation to obtain a sum.\n",
    "\n",
    "### $log(L(Y = y|\\Theta)) = log(\\prod^{n}_{i=1}\\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp(\\frac{-(y_{i}-\\beta_{0}-\\beta_{1}x_{1})^2}{2\\sigma^2}))$\n",
    "\n",
    "### $l(Y = y|\\Theta) = -n(log(\\sigma)+log(2\\pi))-\\frac{1}{2\\sigma^2}\\sum^{n}_{i=1}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2$\n",
    "\n",
    "\n",
    "\n",
    "We can see that the equation depends only on the parameters of the model: $β0$ and $β1$ and we still have to find the values for which $-\\sum^{n}_{i=1}(y_{i}-\\beta_{0}-\\beta_{1}x_{i})^2$ is maximal.\n",
    "\n",
    "\n",
    "### The method of least squares\n",
    "\n",
    "#### Definition\n",
    "\n",
    "You are probably wondering how we know that the line in our model is the one that is the \"closest\" to each of the points in our dataset. Well, it's because of the \"least squares\" method. We're not going to go too far in demonstrating the formula. What you have to understand is that the algorithm will look for the minimum possible distance between each point in your graph using this formula:\n",
    "\n",
    "### $Min (\\sum_{i=0}^{n}(y_{i}-\\hat{y}_{i})^2)$\n",
    "\n",
    "### $Min (\\sum_{i=1}^{n}(y_{i}-\\beta{0}-\\beta_{1}x_{i})^2)$\n",
    "\n",
    "\n",
    "In the case of simple linear regression, maximum likelihood or least square estimation is equivalent to finding the extreme value of the same equation.\n",
    "\n",
    "In this equation, $y_{i}$ represents each individual (or point) in your dataset while  \n",
    "$\\hat{y}_{i}$ represents your model's prediction.\n",
    "\n",
    "After several iterations, your algorithm is able to find the minimum number in this formula and thus have the best possible line that describes your dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In python\n",
    "\n",
    "Statsmodels is a python package that contains most of the statistical models we will use in this course. We import it with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"A\": [1, 2, 3, 4],\n",
    "    \"B\": [12, 23, 34, 11]\n",
    "})\n",
    "\n",
    "X = df[\"A\"].values\n",
    "y = df[\"B\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the parameters of the model by using the method of least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a summary of the model parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/opt/anaconda3/lib/python3.7/site-packages/statsmodels/stats/stattools.py:71: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n  \"samples were given.\" % int(n), ValueWarning)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.711\nModel:                            OLS   Adj. R-squared (uncentered):              0.615\nMethod:                 Least Squares   F-statistic:                              7.394\nDate:                Thu, 11 Jun 2020   Prob (F-statistic):                      0.0726\nTime:                        19:03:16   Log-Likelihood:                         -15.569\nNo. Observations:                   4   AIC:                                      33.14\nDf Residuals:                       3   BIC:                                      32.52\nDf Model:                           1                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             6.8000      2.501      2.719      0.073      -1.158      14.758\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.641\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.691\nSkew:                          -0.929   Prob(JB):                        0.708\nKurtosis:                       2.165   Cond. No.                         1.00\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.711</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.615</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   7.394</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 11 Jun 2020</td> <th>  Prob (F-statistic):</th>           <td>0.0726</td> \n</tr>\n<tr>\n  <th>Time:</th>                 <td>19:03:16</td>     <th>  Log-Likelihood:    </th>          <td> -15.569</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>     4</td>      <th>  AIC:               </th>          <td>   33.14</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>     3</td>      <th>  BIC:               </th>          <td>   32.52</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>              <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>x1</th> <td>    6.8000</td> <td>    2.501</td> <td>    2.719</td> <td> 0.073</td> <td>   -1.158</td> <td>   14.758</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   1.641</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   0.691</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.929</td> <th>  Prob(JB):          </th> <td>   0.708</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.165</td> <th>  Cond. No.          </th> <td>    1.00</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize model and datas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"235.34pt\" version=\"1.1\" viewBox=\"0 0 352.7 235.34\" width=\"352.7pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 235.34 \nL 352.7 235.34 \nL 352.7 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 224.64 \nL 345.5 224.64 \nL 345.5 7.2 \nL 10.7 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mc7eef62055\" style=\"stroke:#000000;\"/>\n    </defs>\n    <g clip-path=\"url(#p9f7f26a2b9)\">\n     <use style=\"stroke:#000000;\" x=\"26.821874\" xlink:href=\"#mc7eef62055\" y=\"176.985148\"/>\n     <use style=\"stroke:#000000;\" x=\"127.673958\" xlink:href=\"#mc7eef62055\" y=\"97.084501\"/>\n     <use style=\"stroke:#000000;\" x=\"228.526042\" xlink:href=\"#mc7eef62055\" y=\"17.183853\"/>\n     <use style=\"stroke:#000000;\" x=\"329.378126\" xlink:href=\"#mc7eef62055\" y=\"184.248844\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\"/>\n   <g id=\"matplotlib.axis_2\"/>\n   <g id=\"line2d_1\">\n    <path clip-path=\"url(#p9f7f26a2b9)\" d=\"M 26.821874 214.756364 \nL 127.673958 165.363236 \nL 228.526042 115.970108 \nL 329.378126 66.576981 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:3;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 224.64 \nL 10.7 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 345.5 224.64 \nL 345.5 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 224.64 \nL 345.5 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 7.2 \nL 345.5 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9f7f26a2b9\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"10.7\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, y,  color='black')\n",
    "plt.plot(X, predictions, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple linear regression\n",
    "\n",
    "#### Definition\n",
    "\n",
    "##### Plurality of independent variables\n",
    "\n",
    "Most of the time, you will not have just one factor that will allow you to predict your dependent variable. For example, you can predict someone's salary with the number of years of experience, but you can also predict the type of degree, the sector in which the person works, the gender, the country, and so on.\n",
    "\n",
    "This is the only difference between single and multiple linear regression. You add independent variables into the equation.\n",
    "\n",
    "\n",
    "\n",
    "### Normalizing the variables\n",
    "\n",
    "It is essential to normalize the explanatory variables before calculating a model, otherwise the explanatory variables with the largest amplitudes will naturally become more important than the others. Therefore, the variables must be transformed so that their values are comparable and their mean is zero (this is called reduction centering).\n",
    "\n",
    "In python we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random features\n",
    "import numpy as np \n",
    "X = np.random.randn(4, 2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical notation of the problem\n",
    "\n",
    "By noting $Y$ the target variable, $X_1, X_2, ..., X_n$ the explanatory variables, $\\beta_{0}$ the model parameters and $\\epsilon$ the vector of residuals, the multiple linear regression model is written :\n",
    "\n",
    "### $Y_{i} = \\beta_{0}+X_{i,1}\\beta_{1}+...+X_{i,p}\\beta_{p}+\\epsilon_{i}\\forall i \\in \\left [1,n \\right ]$\n",
    "\n",
    "\n",
    "You can also write the problem in matrix form as follows:\n",
    "\n",
    "### $Y=X\\beta+\\epsilon$\n",
    "\n",
    "\n",
    "Where $Y$ is a vector of $n*1$ dimensions, $X$ is a matrix of $n * p$ dimensions, $\\beta_{0}$ is a vector of $p * 1$ dimensions and $\\epsilon$ is a vector of $n * 1$ dimensions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance/Covariance Matrix\n",
    "\n",
    "The variance/covariance matrix of a collection of $p$ random variables indexed by $n_{i}$ from 1 to $p$ is a square matrix of size $p * p$ whose elements are :\n",
    "\n",
    "\n",
    "### $\\sigma_{ii} = Var(X_{i})$\n",
    "\n",
    "\n",
    "The elements along the diagonal of the variance/covariance matrix are the respective variances of each random variable. The other elements are :\n",
    "\n",
    "\n",
    "### $\\sigma_{ij} = Cov(X_{i}, X_{j}) = E\\left [(X_{i}-E(X_{j}))(X_{j}-E(X_{j}))) \\right ], i \\neq j$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The covariances between the different random variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix resolution by maximum likelihood\n",
    "\n",
    "The calculation of the maximum likelihood estimator is a classic exercise in statistics, so we present the calculation here for those who are familiar with or would like to become familiar with matrix calculation.\n",
    "\n",
    "\n",
    "### $L(Y = y|\\beta) = f(Y=y|\\beta)$\n",
    "\n",
    "### $L(Y = y|\\beta) = f(X\\beta+\\epsilon = y|\\beta)$\n",
    "\n",
    "### $L(Y = y|\\beta) = f(\\epsilon = y-X\\beta|\\beta)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "According to the assumptions necessary to be able to use a multiple linear model, $\\epsilon$ follows a centered Normal distribution $(E(\\epsilon) = 0*n)$ and diagonal covariance matrix $\\sum = Diag(\\sigma_{1}^2, \\sigma_{2}^{2}, ... , \\sigma_{p}^{2})$. Which brings us to the following equation:\n",
    "\n",
    "### $L(Y = y|\\beta) = det(2\\pi\\sum)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}((y=X\\beta)^{t}\\sum^{-1}(y-X\\beta)))$\n",
    "\n",
    "\n",
    "$\\sum$ is a diagonal matrix, hence:\n",
    "\n",
    "### $L(Y = y|\\beta) = det(2\\pi\\sum)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}\\sum^{-1}((y=X\\beta)^{t}(y-X\\beta)))$\n",
    "\n",
    "\n",
    "We apply the logarithm which is increasing and therefore does not change the optimization problem under consideration:\n",
    "\n",
    "### $log(L(Y = y|\\beta)) = -\\frac{1}{2}log(det(2\\pi\\sum))-\\frac{1}{2}\\sum^{-1}(y-X\\beta)^{t}(y-X\\beta)$\n",
    "\n",
    "\n",
    "\n",
    "We try to find the value of $\\beta$ that maximizes the above equation, which is like finding the minimum of the following value:\n",
    "\n",
    "### $\\underset{\\beta}{min}$ $ (y-X\\beta)^{t}(y-X\\beta) = \\beta_{MLE}$\n",
    "\n",
    "### $\\underset{\\beta}{min}$ $ y^{t}y-\\beta^{t}X^{t}y-y^{t}X\\beta+\\beta^{t}X^{t}X\\beta = \\beta_{MLE}$\n",
    "\n",
    "\n",
    "\n",
    "$y^{t}X\\beta$ is a scalar (i.e. a real number of dimension 1), so it is equal to its transpose $y^{t}X\\beta = \\beta^{t}X^{t}y$!, hence:\n",
    "\n",
    "### $\\underset{\\beta}{min}$ $ y^{t}y-2\\beta^{t}X^{t}y + \\beta^{t}X^{t}X\\beta = \\beta_{MLE}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We derive from $\\beta$ and we get:\n",
    "\n",
    "### $-X^{t}y + X^{t}X\\beta = 0 \\Rightarrow \\beta_{MLE} = (X^{t}X)^{-1}X^{t}y$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This solution is well defined only if $X^{t}X$ is an invertible matrix, which is true if the explanatory variables are not collinear and if $p < n$ (the number of explanatory variables is less than the number of observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the calculation of the model is done using the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is indeed the same command as for simple linear regression (because it is the same model!) the only thing that changes is the shape of X!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions for multiple linear regression\n",
    "\n",
    "### Everything in simple linear regression plus NO collinearity\n",
    "\n",
    "As you can already imagine, multiple linear regressions will follow the same assumptions as simple linear regressions because, after all, you just add a bit of complexity. The only thing you need to add in the assumptions is the _non-colinearity_ of the independent variables.\n",
    "\n",
    "For example, if you are trying to predict someone's salary based on their age and years of experience, you will run into a problem. The relationship between age and years of experience is quite possible, since logically, the older you are, the more years of experience you have.\n",
    "\n",
    "If you have collinearity in your model, your model will be biased and unusable because we won't know which variable really influences your dependent variable.   \n",
    "\n",
    "\n",
    "\n",
    "### Dummy Variables\n",
    "#### Reminder: Categorical variables\n",
    "\n",
    "To understand what dummy variables are, let us first recall what categorical variables are: they are simply qualitative data. For example, think about countries, shoe sizes, etc. You could technically create a category for each variable.\n",
    "\n",
    "\n",
    "#### Encode categorical variables\n",
    "\n",
    "In regressions, you cannot have text data as variables, only numbers are accepted. This is why categorical variables are encoded and replaced by 0 or 1.\n",
    "\n",
    "\n",
    "\n",
    "#### The dummy variable trap\n",
    "\n",
    "Once you have encoded your dummy variables, you won't add them all into your equation because you will have a collinearity problem between your last dummy variable and your first one: one will be the opposite of the other. So you will add all the dummy variables and **removing 1 in your equation.**\n",
    "\n",
    "Indeed, one of the hypotheses of the multiple linear regression model is the non-collinearity of the explanatory variables: there should be no linear combination that links the variables together.\n",
    "\n",
    "A linear combination between variables $X_{1}, X_{2}, ..., X_{m}$ is a sum of the form :\n",
    "\n",
    "### $a_{1}X_{1}+a_{2}X_{2}+...+a_{m}X_{m}+a_{0}$\n",
    "\n",
    "\n",
    "Where the coefficients $a_{0}, a_{1}, ..., a_{m}$ are real numbers. Variables are said to be linearly related, or collinear, if there is a group of coefficients such as :\n",
    "\n",
    "### $a_{1}X_{1}+a_{2}X_{2}+...+a_{m}X_{m}+a_{0} = 0$\n",
    "\n",
    "\n",
    "\n",
    "In the case of dummy variables, the sum of all variables is always 1, because each observation necessarily belongs to one of the modalities, in fact :\n",
    "\n",
    "### $modalitie_{1} + modalitie_{2}+...+modalitie_{m}-1 = 0$\n",
    "\n",
    "\n",
    "\n",
    "And the variables are collinear. It is therefore essential to remove one of the dummy variables in order to avoid collinearity of the variables, the removed variable will correspond to the default modality, and the coefficients associated with the other variables will represent the influence of each modality with respect to the default level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform simple encoding of categorical variables the following commands can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Age  Colors_Blue  Colors_Green  Colors_Red\n0   10            0             0           1\n1   12            0             0           1\n2   32            1             0           0\n3   21            0             1           0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Colors_Blue</th>\n      <th>Colors_Green</th>\n      <th>Colors_Red</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>32</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "X = pd.DataFrame({\n",
    "    \"Colors\": [\"Red\", \"Red\", \"Blue\", \"Green\"],\n",
    "    \"Age\": [10, 12, 32, 21]\n",
    "})\n",
    "\n",
    "pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable selection and model selection\n",
    "\n",
    "The main feature of multiple linear regression is that it uses several explanatory variables together. Therefore, the natural question is: which variables should I use to build the best possible model for my objectives? This question leads us to introduce model evaluation criteria and variable selection methods.\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation of multiple linear regression models\n",
    "\n",
    "Some of the evaluation criteria presented below may be used for models other than multiple linear regression. It is therefore all the more important to introduce them now and to remember their respective interpretations.\n",
    "\n",
    "\n",
    "\n",
    "#### Analysis of Variance (ANOVA)\n",
    "\n",
    "The analysis of variance allows to quantify the performance of a statistical model in terms of estimation error. The different values that we will discuss now will be used to construct other indicators:\n",
    "\n",
    "\n",
    "\n",
    "* SST: Sum of Square Total is an indicator of the dispersion of the values of the target variable $Y$ (whose values are noted $y_{1}, ..., y_{n}$) over the population considered, which is written mathematically :\n",
    "\n",
    "### $SST$ $ = \\sum_{i=1}^{n}(y_{i}-\\bar{y})^2$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It is the sum of the deviations from the squared mean of the values taken by the target variable $Y$ for the $n$ observations considered.\n",
    "\n",
    "* SSE: Sum of Square Explained is an indicator that represents the amount of dispersion of the target variable that is explained by the model, which is written:\n",
    "\n",
    "### $SSE$ $ = \\sum_{i=1}^{n}(\\hat{y}_{i}-\\bar{y})^2$\n",
    "\n",
    "\n",
    "It is the sum of the squared mean differences between the model estimates for each observation and the mean of the target variable for the population of interest.\n",
    "\n",
    "* SSR: Sum of Squared Residual is an indicator that quantifies the error committed by the model, or in other words the portion of the dispersion of the target variable that is not explained by the model, hence the idea of residual. Its formula is as follows:\n",
    "\n",
    "### $SSR$ $ =\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2$\n",
    "\n",
    "\n",
    "\n",
    "It is essential to understand these values because they will allow us to build all the evaluation metrics for the multiple linear regression models we will see now.\n",
    "\n",
    "To summarize, SST is the total variance of the target variable, which can be decomposed into two components : SSE is the variance explained by the model, which is the amount of variance of our estimates relative to the actual mean of the observed population, and SSR is the sum of the squares of the differences between our estimates is the actual values of the target variable. In other words, SST is the total amount of information, SSE is the information explained by the model and SSR is the information that remains to be explained, or the error committed.\n",
    "\n",
    "\n",
    "\n",
    "#### F-Statistics by Fisher\n",
    "\n",
    "A statistical test is a process by which we try to show whether a hypothesis is confirmed or disproved by the data at our disposal. This test hypothesis, also called null hypothesis and noted $H_{\\varnothing}$, would have consequences on the properties of the observed data if it is actually verified. These properties are summarized by a test statistic, the value of which gives an idea of the probability that H₀ is true.\n",
    "\n",
    "Fisher's F-statistic allows to test the veracity of the following hypotheses:\n",
    "\n",
    "\n",
    "* When the Fisher test is applied to the model as a whole, the null hypothesis, noted $H_{\\varnothing}$, is \"the variables chosen to construct the model are not jointly significant in describing the target variable\". If the hypothesis is true, the F-statistic should follow a probability distribution law F of parameters $(n - 1, n - 1)$where $n$ is the number of observations used to construct the model. However, if the value of the F-statistic, denoted \"F\", is outside the most probable regions of the distribution, then we can reject the null hypothesis and conclude that the chosen model has a real explanatory power on the target variable.\n",
    "\n",
    "Mathematically, the F-statistic is written:\n",
    "\n",
    "### $F$ $ = \\frac{SSE}{SSR}$\n",
    "\n",
    "\n",
    "\n",
    "The F-test can also compare two nested models (model 1 which includes \"explanatory variables\" and model 2 which includes \"explanatory variables\" including the \"variables of model 1 and $d < of$. In this case the F-statistic follows an F-law of parameters $(n - 1, n - 1)$ if the assumption that the simplest model (model 2) of the two models best describes the target variable is verified. The mathematical formula of F is then :\n",
    "\n",
    "### $F$ $ = (\\frac{SSR_{1}-SSR_{2}}{p_{1}-p_{2}})(\\frac{n-p_{2}}{SSR_{2}})$\n",
    "\n",
    "\n",
    "\n",
    "If the value of F is in an unlikely region of the F-distribution it is supposed to follow, then the hypothesis is rejected and the test suggests that the more complex Model 2 provides significant additional information compared to the simpler Model 1.\n",
    "\n",
    "\n",
    "Graphically the F-test can be illustrated as follows:\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1KKDqtiiimdfHZK59JkZ2wyvDOQUJA8ER)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In black, we represent the density of the F law, as in any test we define a level $a$ between 0 and 1 which will influence the size of the hypothesis rejection zone. Very often, we take $α = 5%$ when no business knowledge can help us to modulate our precision requirements. The F test is one-sided, only large values of F will allow us to reject the hypothesis. More precisely if the value of F is in the upper part of the expected distribution equivalent to 5% probability, then we can say that the hypothesis is rejected at $1 - α$ 95%.\n",
    "\n",
    "\n",
    "This first metric allows us to test the hypothesis that the explanatory variables have no influence on the target variable, we will now look at metrics that indicate the performance of the model.\n",
    "\n",
    "\n",
    "\n",
    "* $R^{2}$ (R square)\n",
    "\n",
    "\n",
    "$R^{2}$, or R-squared, is a statistic that quantifies the explanatory power of the model with respect to the target variable.\n",
    "\n",
    "### $R^{2}$ $ = 1-\\frac{SSR}{SST}$\n",
    "\n",
    "\n",
    "R² is monotonically increasing with the number of explanatory variables added to the model. It varies between 0 and 1, if the model is not very relevant, the sum of the residual squares $SSR$ will be close to the sum of the total squares $SST$ and $R^{2}$ will be closer to 0, on the contrary, if the model allows to explain the target variable faithfully, then $SSR$ will be closer to 0 and $R^2$ will be closer to 1. So mechanically, with each addition of variable to the model, the prediction of $Y$, the target variable, will be better and $R^2$ will be higher. In fact, $R^2$ is a performance indicator that only allows to compare two models that have the same number of explanatory variables.\n",
    "\n",
    "\n",
    "\n",
    "* $R^2 - adjusted$\n",
    "\n",
    "\n",
    "$R² - adjusted$ is a modified version of $R²$ that penalizes the number of explanatory variables selected to build the model. Its mathematical formula is:\n",
    "\n",
    "### $R^2 - ajusted$ $ = 1-\\frac{n-1}{n-p-1}(1-R^2)$\n",
    "\n",
    "\n",
    "Where $R^2$ is the number of explanatory variables used and $n$ is the number of observations used. The growth of $R^2$ as a function of $p$ is compensated by the decrease of $\\frac{n-1}{(n-p-1)}$ as a function of $p$. Consequently, if the information contribution of an explanatory variable is not significant enough, then $R^2 - adjusted$ will decrease. In fact, it is possible to use this indicator to compare the performance of models that do not necessarily have the same number of explanatory variables.\n",
    "\n",
    "\n",
    "In order to observe all these indicators in python the following command should be used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/opt/anaconda3/lib/python3.7/site-packages/statsmodels/stats/stattools.py:71: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n  \"samples were given.\" % int(n), ValueWarning)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.106\nModel:                            OLS   Adj. R-squared (uncentered):             -0.788\nMethod:                 Least Squares   F-statistic:                             0.1185\nDate:                Thu, 11 Jun 2020   Prob (F-statistic):                       0.894\nTime:                        19:12:07   Log-Likelihood:                         -17.830\nNo. Observations:                   4   AIC:                                      39.66\nDf Residuals:                       2   BIC:                                      38.43\nDf Model:                           2                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1            -0.5015     16.094     -0.031      0.978     -69.750      68.747\nx2             7.3717     16.094      0.458      0.692     -61.877      76.620\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   0.153\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.685\nSkew:                          -0.922   Prob(JB):                        0.710\nKurtosis:                       2.157   Cond. No.                         1.52\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.106</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>  -0.788</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>  0.1185</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 11 Jun 2020</td> <th>  Prob (F-statistic):</th>           <td> 0.894</td> \n</tr>\n<tr>\n  <th>Time:</th>                 <td>19:12:07</td>     <th>  Log-Likelihood:    </th>          <td> -17.830</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>     4</td>      <th>  AIC:               </th>          <td>   39.66</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>     2</td>      <th>  BIC:               </th>          <td>   38.43</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>              <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>x1</th> <td>   -0.5015</td> <td>   16.094</td> <td>   -0.031</td> <td> 0.978</td> <td>  -69.750</td> <td>   68.747</td>\n</tr>\n<tr>\n  <th>x2</th> <td>    7.3717</td> <td>   16.094</td> <td>    0.458</td> <td> 0.692</td> <td>  -61.877</td> <td>   76.620</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   0.153</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   0.685</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.922</td> <th>  Prob(JB):          </th> <td>   0.710</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.157</td> <th>  Cond. No.          </th> <td>    1.52</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the .summary() command is not available in the statsmodels package but not in the sklearn package which is a more machine learning oriented package and not really statistics and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection.\n",
    "\n",
    "When we have at our disposal \"no\" explanatory variables, the number of models that it is possible to construct can be counted in the following way: we consider the variables one by one and we ask ourselves each time if we select it for the model or not, in fact if we have a variable we can construct two models, the one with $X_1$ or the one without $X_1$. If we add more possible explanatory variables, we build like a tree, the first two branches corresponding to the fact of selecting or not selecting the variable $X_1$. They divide themselves into two branches to select or not select $X_2$ and so on. In fact, if we have at our disposal $p$ explanatory variables, we can potentially build $2^p$ models with them. In practice, when the number of $p$ explanatory variables is large, we cannot explore the $2^p$ models that can be constructed using these variables in order to select the best one. Different methods exist which allow to avoid using brute force.\n",
    "\n",
    "\n",
    "\n",
    "        1. Step by step\n",
    "\n",
    "The step-by-step selection is divided into three variants:\n",
    "\n",
    "* Forward selection: the variables are added one by one to the model by selecting at each step the one with the lowest p-value (Fisher's test comparing two nested models). It stops when all the variables are used or if the minimum p-value becomes higher than a threshold value, which by default is set to 0.5.\n",
    "* Elimination (backward): This time we start with a model using all the explanatory variables. At each step, the variable with the highest p-value associated with the Fisher test is eliminated from the model. The procedure stops when all the remaining variables have p-values higher than a threshold set by default at 0.1 (but which can be adapted according to the precision needs of the considered problem).\n",
    "* Stepwise: This algorithm alternates between a selection step and an elimination step after each addition of a variable, in order to remove any variables that would have become less relevant in the presence of those that have been added.\n",
    "\n",
    "\n",
    "\n",
    "        2. By exchange\n",
    "\n",
    "The aim of this method is to find the best model for each level (for each number of independent explanatory variables) starting with level 1. At each level, a variable not yet included in the model is selected that maximizes the increase of $R^2$. Then, it exchanges in turn a variable present in the model with a variable outside the model and keeps the configuration that maximizes $R^2$.\n",
    "\n",
    "The same algorithm can be slightly modified to select the exchange of the two variables that achieves the smallest R² increase, the idea behind this variant is to explore more different models and thus reach a better optimum.\n",
    "\n",
    "\n",
    "\n",
    "        3. Globally\n",
    "\n",
    "Furnival and Wilson's algorithm allows to compare all possible models with the objective of optimizing one of the evaluation criteria among $R^2$ and $R^2 - adjusted$.\n",
    "\n",
    "\n",
    "\n",
    "### Final remarks\n",
    "\n",
    "Linear models are very sensitive to extreme values that may be present in a dataset, so pre-processing your learning base is essential to avoid having your results completely skewed.\n",
    "\n",
    "The model evaluation and selection methods introduced above are perfectly valid for all linear models, as well as the logistic regression that we will discuss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}