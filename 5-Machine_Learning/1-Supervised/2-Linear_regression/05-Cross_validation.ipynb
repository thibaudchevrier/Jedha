{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to protect yourself from overlearning ##\n",
    "\n",
    "Under-learning situations occur rarely in practice, or they are often due to a lack of relevant data or other problems that cannot be solved directly by data sciences. The real enemy of data scientists is overlearning, because it gives the illusion of performance, which is actually a trap!\n",
    "\n",
    "A simple and effective way to avoid this trap is to practice k-fold cross validation. It is a process that consists in choosing an integer k (often 10 is chosen by default), and randomly distributing the observations into k groups of equal size. Then the following method is repeated k times:\n",
    "\n",
    "\n",
    "\n",
    "* We isolate one group i among 10 groups, which we call **test base**, and we gather the 9 others, which we call **learning base**.\n",
    "* Estimate the chosen model using the learning base.\n",
    "* We calculate the error committed by model i on the test basis (group i) and compare it to the error committed on the learning basis after optimization.\n",
    "\n",
    "The comparison of the learning error and the test error allows us to understand the real explanatory power of a model, because it quantifies the performance of the model on unknown data compared to its performance on known data.\n",
    "\n",
    "\n",
    "![over_learning](https://drive.google.com/uc?export=view&id=1Dx7vSLocMMcjiYmowVS2GzKXbvxgkN5l)\n",
    "\n",
    "\n",
    "The figure above illustrates the principle of k-fold cross-validation. Each iteration produces results in terms of test error and learning error that are used to evaluate the model. These errors are usually calculated based on the cost function chosen to optimize the model, or simply on the average of the errors squared. In general, the testing and validation error is expected to be of the same order of magnitude, and it is hoped that the overall error will be reduced relative to the values taken by the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# Create features \n",
    "X = np.random.randn(100, 2)\n",
    "y = np.random.randint(0,2, 100)\n",
    "\n",
    "# Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "# Create predictions\n",
    "y_pred = classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train: [10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [0 1 2 3 4 5 6 7 8 9]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [10 11 12 13 14 15 16 17 18 19]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 30 31 32 33\n 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [20 21 22 23 24 25 26 27 28 29]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57\n 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [30 31 32 33 34 35 36 37 38 39]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 50 51 52 53 54 55 56 57\n 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [40 41 42 43 44 45 46 47 48 49]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [50 51 52 53 54 55 56 57 58 59]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 70 71 72 73 74 75 76 77 78 79 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [60 61 62 63 64 65 66 67 68 69]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 80 81\n 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] Validation: [70 71 72 73 74 75 76 77 78 79]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n 72 73 74 75 76 77 78 79 90 91 92 93 94 95 96 97 98 99] Validation: [80 81 82 83 84 85 86 87 88 89]\nTrain: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] Validation: [90 91 92 93 94 95 96 97 98 99]\n"
    }
   ],
   "source": [
    "''' k fold cross validation'''\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# the code bellow allows to generate 10 splits train/test for the 10-folds cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "      print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# this method allows to obtain fastly precisions's scores\n",
    "# for e = all differents models estimated by cross-validation\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "scores = cross_val_score(classifier, X, y, cv=10)\n",
    "predictions = cross_val_predict(classifier, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Scores for each cross_validation:\n [0.6 0.6 0.4 0.4 0.4 0.3 0.4 0.4 0.7 0.5]\nPredictions for each cross_validation:\n [0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1\n 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1\n 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1]\n"
    }
   ],
   "source": [
    "print(\"Scores for each cross_validation:\\n {}\".format(scores))\n",
    "print(\"Predictions for each cross_validation:\\n {}\".format(predictions))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}