{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## What you'll learn in this course ## \n",
    "\n",
    "Random Forest is the best way to alleviate overfitting and create a powerful robust algorithm based on decision trees. In this course you'll learn: \n",
    "\n",
    "* What is a Random Forest \n",
    "* Bagging Principles \n",
    "\n",
    "\n",
    "## Intuitive understanding \n",
    "\n",
    "The first idea behind the Random Forest is to do a bagging of several random trees. Several prunings of the trees thus built are possible:\n",
    "\n",
    "\n",
    "\n",
    "* One can keep the complete trees and possibly limit the minimum number of observations at the terminal nodes.\n",
    "* Keep at most $q$ leaves or limit the depth of the tree to $q$ node levels.\n",
    "* Adopt the method seen above for a single tree, i.e. build the complete tree then prune by cross validation.\n",
    "\n",
    "In general, the first strategy will be retained, because it represents a good compromise between quality of estimation and quantity of calculations. Each tree thus constructed will have a very low bias and a large variance. However, aggregating the models together helps to reduce this variance. This algorithm is very simple to implement, which is a great advantage, however the number of models to be computed before the test error (also called validation error) stabilizes can be very important. The final model will be large in terms of disk space because it is necessary to store the complete structure of all the trees in order to be able to make predictions. Finally, the multiplication of the number of trees participating in the model makes it more difficult, if not impossible, to interpret the model as was possible with a single tree.\n",
    "\n",
    "The second idea is to improve the bagging method in order to create Random Forests based on data samples that are as \"independent\" as possible. Not only the chance intervenes at the time of the selection of the observations and during the construction of the learning samples, but one will also make intervene the chance in the choice of the explanatory variables retained for each sample on which one will build a random tree.\n",
    "\n",
    "This double randomness in the selection of observations and explanatory variables has several advantages: it makes it possible to approach the hypothesis of independence of the samples, it reduces the number of calculations to be carried out for the construction of each tree and it reduces the risks of errors linked to possible correlations between explanatory variables.\n",
    "\n",
    "Last remark, i.e. $Y$ the target variable and $X_1,...X_p$ the $p$ explanatory variables at our disposal, in general the number of variables we will keep per tree for a classification is $\\sqrt{p}$ and $\\frac{p}{3}$ for a regression.\n",
    "\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "Aggregating the models to produce the final prediction makes direct interpretation of the model difficult, however two methods are used in practice to assess the importance of each variable for the prediction.\n",
    "\n",
    "\n",
    "\n",
    "### Mean decrease Accuracy\n",
    "\n",
    "This method consists in randomly swapping the values of an explanatory variable. The difference between the pre- and post-switching validation error is then measured. The higher the pre-switching validation error, the more important the variable in question is considered to be for the prediction of the target variable.\n",
    "\n",
    "\n",
    "\n",
    "### Mean decrease Gini\n",
    "\n",
    "This method makes it possible to evaluate the importance of a variable at the level of a node: it measures the decrease of the heterogeneity function if we re-use the explanatory variable used for the node by the one we want to evaluate. The general importance of the variable is then a sum of the decreases in heterogeneity measured and weighted by the number of observations at each node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}