{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Yelp\n",
    "\n",
    "The aim of this exercise is to allow a user to make an automatic search on <a href=\"https://www.yelp.fr/\" target=\"_blank\">Yelp</a> and store the results in a `.json` file. You will be guided through the different steps: making a form request with search keywords, parsing the search results, crawling all the result pages and storing the results into a file.\n",
    "\n",
    "⚠ **As scrapy is not made to launch several crawler processes in the same script, you will have to restart your notebook's kernel before completing each question!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a class `YelpSpider(scrapy.Spider)` with `start_urls = ['https://www.yelp.fr/']`. In this class, define a `parse(self, response)` method that automatically fills Yelp's homepage form with: \"restaurant japonais\" as search keywords and \"Paris\" as search location. Then, define another method `after_search(self, response)` that parses the first page of results, and yields the name and url of each search result. Finally, declare a `CrawlerProcess` that will store the results in a file named `\"restaurant_japonais-paris.json\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your class YelpSpider(scrapy.Spider) with all methods needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 14:19:36 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-01 14:19:36 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-09-01 14:19:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-09-01 14:19:36 [scrapy.extensions.telnet] INFO: Telnet Password: 07c0818057376a18\n",
      "2020-09-01 14:19:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-09-01 14:19:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-09-01 14:19:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-09-01 14:19:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-09-01 14:19:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-09-01 14:19:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-09-01 14:19:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-09-01 14:19:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-09-01 14:19:41 [scrapy.extensions.feedexport] INFO: Stored json feed (30 items) in: results/restaurant_japonais-paris.json\n",
      "2020-09-01 14:19:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1929,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 138054,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 4.128968,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 9, 1, 14, 19, 41, 151692),\n",
      " 'item_scraped_count': 30,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 75784192,\n",
      " 'memusage/startup': 75784192,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2020, 9, 1, 14, 19, 37, 22724)}\n",
      "2020-09-01 14:19:41 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# CrawlerProcess and settings go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Once you've managed to get the first page's results in `restaurant_japonais-paris.json`, complete the `after_search(self,response)` method to crawl the different result pages, such that all the search results will be stored in the file `\"restaurant_japonais-paris.json\"`. Restart your notebook's kernel, execute the new `CrawlerProcess` and check that all the search results (and not only the first page) are now stored in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new class YelpSpider based on the previous one \n",
    "# but complete after_search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 14:24:22 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-01 14:24:22 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-09-01 14:24:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-09-01 14:24:22 [scrapy.extensions.telnet] INFO: Telnet Password: d744b107c4cf10b2\n",
      "2020-09-01 14:24:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-09-01 14:24:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-09-01 14:24:23 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-09-01 14:24:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-09-01 14:24:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-09-01 14:24:47 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-09-01 14:24:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-09-01 14:24:47 [scrapy.extensions.feedexport] INFO: Stored json feed (240 items) in: results/restaurant_japonais-paris.json\n",
      "2020-09-01 14:24:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 12467,\n",
      " 'downloader/request_count': 10,\n",
      " 'downloader/request_method_count/GET': 10,\n",
      " 'downloader/response_bytes': 702946,\n",
      " 'downloader/response_count': 10,\n",
      " 'downloader/response_status_count/200': 9,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 24.157405,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 9, 1, 14, 24, 47, 172858),\n",
      " 'item_scraped_count': 240,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75759616,\n",
      " 'memusage/startup': 75759616,\n",
      " 'request_depth_max': 8,\n",
      " 'response_received_count': 9,\n",
      " 'scheduler/dequeued': 10,\n",
      " 'scheduler/dequeued/memory': 10,\n",
      " 'scheduler/enqueued': 10,\n",
      " 'scheduler/enqueued/memory': 10,\n",
      " 'start_time': datetime.datetime(2020, 9, 1, 14, 24, 23, 15453)}\n",
      "2020-09-01 14:24:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# CrawlerProcess and settings go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you've just made the proof of concept of making an automated search on Yelp with Scrapy! Now, let's improve the script such that it will allow the user to make any search at any location 😎\n",
    "\n",
    "3. Use python's `input()` function to ask the user which keywords and location he would like to use, and save them into two variables: `search_keywords` and `search_location`. Then, change the `parse(self, response)` method such that it fills Yelp's form with user-defined keywords and location. Finally, change the `CrawlerProcess` such that it stores the results in a file named with the following format : `search_keywords-location.json`. \n",
    "\n",
    "Try your search engine with different keywords and locations ✌️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in the automated Yelp search engine !\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your search keywords :  spa\n",
      "Please enter the name of the city :  Grenoble\n"
     ]
    }
   ],
   "source": [
    "# Use input() function here to define keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare YelpSpider class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-01 14:31:45 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
      "2020-09-01 14:31:45 [scrapy.utils.log] INFO: Versions: lxml 4.5.2.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 | packaged by conda-forge | (default, Jul 31 2020, 02:39:48) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform Linux-4.19.112+-x86_64-with-glibc2.10\n",
      "2020-09-01 14:31:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2020-09-01 14:31:45 [scrapy.extensions.telnet] INFO: Telnet Password: f7dddfab2d16ca6e\n",
      "2020-09-01 14:31:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-09-01 14:31:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-09-01 14:31:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-09-01 14:31:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-09-01 14:31:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-09-01 14:31:50 [root] INFO: No next page. Terminating crawling process.\n",
      "2020-09-01 14:31:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-09-01 14:31:50 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: results/spa-Grenoble.json\n",
      "2020-09-01 14:31:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1986,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 121408,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 4.849391,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 9, 1, 14, 31, 50, 872261),\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 75993088,\n",
      " 'memusage/startup': 75993088,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2020, 9, 1, 14, 31, 46, 22870)}\n",
      "2020-09-01 14:31:50 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# CrawlerProcess and settings go here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
