{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Projects\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/mlflow-project.png\" width=600>\n",
    "\n",
    "## What you will learn in this course \n",
    "\n",
    "What is very cool is to have a standard way of organizing your ML projects so that you can implement trainings easily. MLFlow projects lets you do that. In this course, you will learn: \n",
    "\n",
    "* What is MLFlow projects\n",
    "* How to organize an MLFlow projects\n",
    "* Understand config files in MLFlow projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MLFlow Projects? 🤔🤔\n",
    "\n",
    "MLFlow Project is a way for you to standardize your projects so that you can use them with any types of technologies and train your models remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is structured an MLFlow Project 🗂️🗂️\n",
    "\n",
    "Now that you registered your metrics and your model, your project should look something like this: \n",
    "\n",
    "```shell \n",
    "├── mlruns\n",
    "│   └── 0\n",
    "│       ├── 0a2b502f674949b4acb8dfce6549a7fb\n",
    "│       │   ├── artifacts\n",
    "│       │   │   └── model\n",
    "│       │   │       ├── MLmodel\n",
    "│       │   │       ├── conda.yaml\n",
    "│       │   │       └── model.pkl\n",
    "│       │   ├── meta.yaml\n",
    "│       │   ├── metrics\n",
    "│       │   │   └── Accuracy\n",
    "│       │   ├── params\n",
    "│       │   │   └── C\n",
    "│       │   └── tags\n",
    "│       │       ├── mlflow.log-model.history\n",
    "│       │       ├── mlflow.source.name\n",
    "│       │       ├── mlflow.source.type\n",
    "│       │       └── mlflow.user\n",
    "│       └── meta.yaml\n",
    "├── train.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this structure what is actually important to understand is: \n",
    "\n",
    "- `artifacts` folder: where you store informations about your model to deploy it.\n",
    "- `meta.yaml` file: where you have all the information regarding your run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts 🏛️🏛️\n",
    "\n",
    "Artifacts are the place where you have all the information regarding the environment when your model has been trained. Especially, you have three files: \n",
    "\n",
    "* `MLModel`\n",
    "* `conda.yml`\n",
    "* `model.pkl`, if you persisted a sklearn model. But you can have other types of files if you persisted a TensorFlow, Pytorch or any other type of model.\n",
    "\n",
    "### `MLModel` \n",
    "\n",
    "An `MLModel` file should look something like this: \n",
    "\n",
    "```yaml\n",
    "artifact_path: model\n",
    "flavors:\n",
    "  python_function:\n",
    "    data: model.pkl\n",
    "    env: conda.yaml\n",
    "    loader_module: mlflow.sklearn\n",
    "    python_version: 3.7.3\n",
    "  sklearn:\n",
    "    pickled_model: model.pkl\n",
    "    serialization_format: cloudpickle\n",
    "    sklearn_version: 0.23.1\n",
    "run_id: 0a2b502f674949b4acb8dfce6549a7fb\n",
    "utc_time_created: '2020-06-14 17:23:39.122114'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives all necesaries informations to run your model. Especially be careful with: \n",
    "\n",
    "- `env`: by default you'll get a `conda` environment but you can setup a `Docker` environment,\n",
    "- `sklearn_version`: be really careful with the versions registered here as it might not be available in your servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `conda.yaml`\n",
    "\n",
    "As stated before, MLFlow Project will create a conda environment so that you can run your project on any server. A `conda.yaml` look like this: \n",
    "\n",
    "```yaml\n",
    "channels:\n",
    "- defaults\n",
    "dependencies:\n",
    "- python=3.7.3\n",
    "- scikit-learn=0.23.1\n",
    "- pip\n",
    "- pip:\n",
    "  - mlflow\n",
    "  - cloudpickle==1.4.1\n",
    "name: mlflow-env\n",
    "```\n",
    "\n",
    "As you can see, you have all the dependencies stated here. Again be careful with versions stated in your YAML file as some servers might not be able to run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote training 📺📺\n",
    "\n",
    "Now that we have a better understanding of our project. We can easily train a model remotely simply by organizing our project the following way:\n",
    "\n",
    "```shell \n",
    "├── MLProject\n",
    "├── conda.yaml\n",
    "└── train.py\n",
    "```\n",
    "\n",
    "### Entry Point file \n",
    "\n",
    "`train.py` is called the entry point file. You need to have a script or an `.sh` file because notebooks are not accepted. Therefore you need to have your whole training process into your own file. The content look like this:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = pd.DataFrame(data = iris[\"data\"], columns= iris[\"feature_names\"])\n",
    "y = pd.DataFrame(data = iris[\"target\"], columns=[\"target\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Mlflow tracking\n",
    "import sys \n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Specified Parameters \n",
    "    c = float(sys.argv[2]) if len(sys.argv) > 1 else 0.5 # Let the user specify C argument via Cli\n",
    "\n",
    "    # Instanciate and fit the model \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # Store metrics \n",
    "    predicted_qualities = lr.predict(X_test.values)\n",
    "    accuracy = lr.score(X_test.values, y_test.values)\n",
    "\n",
    "    # Print results \n",
    "    print(\"LogisticRegression model\")\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "\n",
    "    # Log Metric \n",
    "    mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "\n",
    "    # Log Param\n",
    "    mlflow.log_param(\"C\", c)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `conda.yaml`\n",
    "\n",
    "`conda.yaml` won't change from what we showed you above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MLProject`\n",
    "\n",
    "In your MLProject file, you can specify few additionnal things like let the user specify some parameters and you will need to specify to you `entry_points.py`:\n",
    "\n",
    "```yaml\n",
    "artifact_path: model\n",
    "flavors:\n",
    "  python_function:\n",
    "    data: model.pkl\n",
    "    env: conda.yaml\n",
    "    loader_module: mlflow.sklearn\n",
    "    python_version: 3.7.3\n",
    "  sklearn:\n",
    "    pickled_model: model.pkl\n",
    "    serialization_format: cloudpickle\n",
    "    sklearn_version: 0.23.1\n",
    "run_id: 0a2b502f674949b4acb8dfce6549a7fb\n",
    "utc_time_created: '2020-06-14 17:23:39.122114'\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      c: {type: float, default: 0.8}\n",
    "    command: \"python train.py -c {c}\"\n",
    "```\n",
    "\n",
    "As you can see in the `entry_points:` section we specified some `parameters` and provide the command `python train.py -c` which will be the command that will be run when we'll be calling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a training process 🏃‍♀️🏃‍♀️\n",
    "\n",
    "Now finally, if you have all these files ready, you can simply run a CLI command from any server: \n",
    "\n",
    "```shell \n",
    "$ mlflow run path_to_your_project\n",
    "```\n",
    "\n",
    "> NB: if you are already in the project folder, you can simply do:</br>\n",
    "> ```shell \n",
    "> $ mlflow run . \n",
    "> ```\n",
    "\n",
    "To add parameters:\n",
    "```shell\n",
    "$ mlflow run path_to_your_project -P c=0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "* <a href=\"https://mlflow.org/docs/latest/projects.html\" target=\"_blank\">Mlflow Projects</a>\n",
    "* <a href=\"https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html\" target=\"_blank\">Mflow Tutorial</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
